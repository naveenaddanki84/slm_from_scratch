{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "961xR8LdcJQY"
   },
   "outputs": [],
   "source": [
    "# Small LLM / Notebook created by Javier Ideami (ideami.com)\n",
    "# Typical LLMs need many GPUs and millions of dollars to be trained\n",
    "# This code trains a small LLM with a single GPU and little GPU memory \n",
    "# Of course results are not like a chatGPT, but they are good enough to see how the LLM trains to go\n",
    "# from random combinations of letters to actual words and phrases that are sometimes decently coherent\n",
    "# GPT3 has 175 Billion parameters. GPT4 has many, many more.\n",
    "# This model has only 19 Million parameters with its default settings. That's why its perfect for learning \n",
    "# and experimenting\n",
    "\n",
    "# Official notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1tUgAJccK6D",
    "outputId": "dbabcd5d-ad95-4518-a0f8-e04fddbce82c"
   },
   "outputs": [],
   "source": [
    "# uncomment and run the following installation lines ONLY if you havent installed these libraries already outside of the notebook\n",
    "#!pip install ipdb -q\n",
    "#!pip install tqdm -q\n",
    "#!pip install sentencepiece -q\n",
    "#!pip install wandb -q\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this command to view information about your GPU and the amount of free memory it has\n",
    "# Make sure that you have at last 4GB of free GPU memory to do this course\n",
    "!nvidia-smi \n",
    "# If you are using Google Colab or a similar online platform, make sure to select a GPU in the menus\n",
    "# In Google colab, at the moment the option is within the Runtime menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VnNqwkhiU3n"
   },
   "outputs": [],
   "source": [
    "### Import necessary libraries\n",
    "\n",
    "# Standard Python libraries for file operations and system utilities\n",
    "import os, sys  # Operating system interface and system-specific parameters\n",
    "import ipdb  # Interactive Python debugger - allows us to pause execution and inspect variables\n",
    "from tqdm import tqdm  # Progress bar library - shows training progress with visual bars\n",
    "from datetime import datetime  # Date and time utilities for logging timestamps\n",
    "import platform, shutil  # Platform detection and file operations\n",
    "\n",
    "# HTTP and file handling libraries\n",
    "import requests  # For downloading datasets and model files from URLs\n",
    "import zipfile, io  # For extracting compressed files and handling binary data\n",
    "\n",
    "# PyTorch - The main deep learning framework we'll use\n",
    "import torch  # Core PyTorch library for tensor operations and GPU acceleration\n",
    "import torch.nn as nn  # Neural network modules (layers, activations, etc.)\n",
    "from torch.nn import functional as F  # Functional interface for neural network operations\n",
    "\n",
    "# Tokenization library - converts text into numbers that neural networks can understand\n",
    "import sentencepiece as spm  # Google's SentencePiece tokenizer for subword tokenization\n",
    "\n",
    "# Performance optimizations for modern GPUs (Ampere architecture like A100, RTX 30/40 series)\n",
    "# TF32 (Tensor Float 32) is a faster but slightly less precise format than FP32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for matrix multiplications (faster training)\n",
    "torch.backends.cudnn.allow_tf32 = True  # Enable TF32 for cuDNN operations (faster convolutions)\n",
    "\n",
    "# Clear GPU memory to start with a clean slate\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5q26l98govJ"
   },
   "outputs": [],
   "source": [
    "# Download necessary files and create necessary folders\n",
    "# wiki.txt - dataset: a tiny segment of the English Wikipedia\n",
    "# wiki_tokenizer.model: trained tokenizer file (in another notebook I show you how to produce this file)\n",
    "# wiki_tokenizer.vocab: trained tokenizer file (in another notebook I show you how to produce this file)\n",
    "# encoded_data.pt (dataset tokenized with the tokenizer)\n",
    "# I will explain how to produce encoded_data.pt - because it takes quite a bit to process, it's nice to have it in advance\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created. If you have any problems downloading the files with this code, I have also added llm_train.zip\n",
    "# to the downloadable resources of this lecture (however, best option is to use this code, because then you don't need\n",
    "# to upload the files or do anything else)\n",
    "\n",
    "files_url = \"https://ideami.com/llm_train\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"encoded_data.pt\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the encoded_data.pt file\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fre7fXD0fVD9",
    "outputId": "04d590af-d8cc-4e93-fd10-60b97fc473d1"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION PARAMETERS - The Heart of Our LLM Training Setup\n",
    "# =============================================================================\n",
    "# This section defines all the key parameters that control our model architecture,\n",
    "# training process, and performance. Understanding these parameters is crucial for\n",
    "# building and training effective language models.\n",
    "\n",
    "# =============================================================================\n",
    "# ARCHITECTURE PARAMETERS - Model Structure and Size\n",
    "# =============================================================================\n",
    "# These parameters define the physical structure of our transformer model\n",
    "\n",
    "batch_size = 8  # Number of training samples processed simultaneously\n",
    "                # Higher batch sizes = faster training but more GPU memory required\n",
    "                # 8 is optimal for 4GB GPU, 128 for 24GB GPU\n",
    "                # Batch size affects gradient stability and training speed\n",
    "\n",
    "context = 512    # Maximum sequence length the model can process (context window)\n",
    "                # Longer contexts = better understanding but more memory/computation\n",
    "                # 512 is a good balance between performance and resource usage\n",
    "                # GPT-3 uses 2048, GPT-4 uses 8192+ tokens\n",
    "\n",
    "embed_size = 384  # Dimension of token embeddings (how \"rich\" each token representation is)\n",
    "                  # Higher dimensions = more expressive but more parameters\n",
    "                  # 384 is a good middle ground for our 19M parameter model\n",
    "                  # GPT-3 uses 12288 dimensions\n",
    "\n",
    "n_layers = 7      # Number of transformer blocks (depth of the network)\n",
    "                  # More layers = more complex reasoning but more parameters\n",
    "                  # 7 layers is sufficient for our small model\n",
    "                  # GPT-3 has 96 layers, GPT-4 has 120+ layers\n",
    "\n",
    "n_heads = 7       # Number of attention heads per layer\n",
    "                  # More heads = more parallel attention patterns\n",
    "                  # Must divide evenly into embed_size (384/7 ≈ 54.8, so we use 7)\n",
    "                  # GPT-3 uses 96 heads, GPT-4 uses 128+ heads\n",
    "\n",
    "BIAS = True       # Whether to include bias terms in linear layers\n",
    "                  # Bias terms help the model learn offset values\n",
    "                  # Generally recommended for better performance\n",
    "\n",
    "# =============================================================================\n",
    "# HYPERPARAMETERS - Training Behavior and Optimization\n",
    "# =============================================================================\n",
    "# These parameters control how the model learns and optimizes\n",
    "\n",
    "lr = 3e-4        # Learning rate - how big steps the optimizer takes\n",
    "                 # Too high = unstable training, too low = slow learning\n",
    "                 # 3e-4 is a good starting point for transformer models\n",
    "                 # GPT-3 uses 6e-4, but we use slightly lower for stability\n",
    "\n",
    "dropout = 0.05   # Fraction of neurons randomly set to zero during training\n",
    "                 # Prevents overfitting by adding randomness\n",
    "                 # 0.05 = 5% dropout rate (relatively low for transformers)\n",
    "                 # GPT-3 uses 0.1, but we use less for our smaller model\n",
    "\n",
    "weight_decay = 0.01  # L2 regularization strength\n",
    "                     # Penalizes large weights to prevent overfitting\n",
    "                     # 0.01 is a moderate regularization strength\n",
    "                     # Helps the model generalize better to unseen data\n",
    "\n",
    "grad_clip = 1.0      # Maximum gradient norm before clipping\n",
    "                     # Prevents gradient explosion during training\n",
    "                     # 1.0 is a standard value that works well\n",
    "                     # Essential for stable training of deep networks\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING PARAMETERS - Training Process Control\n",
    "# =============================================================================\n",
    "# These parameters control the training loop and evaluation\n",
    "\n",
    "train_iters = 100000  # Maximum number of training iterations\n",
    "                      # Each iteration processes one batch\n",
    "                      # 100k iterations should be sufficient for our small model\n",
    "                      # GPT-3 was trained for 300B tokens (much more)\n",
    "\n",
    "eval_interval = 50    # How often to evaluate the model (every N iterations)\n",
    "                      # More frequent = better monitoring but slower training\n",
    "                      # 50 iterations is a good balance\n",
    "                      # Allows us to track training progress\n",
    "\n",
    "eval_iters = 3        # Number of batches to use for evaluation\n",
    "                      # More batches = more accurate evaluation but slower\n",
    "                      # 3 batches gives a good estimate of model performance\n",
    "                      # Balances accuracy with speed\n",
    "\n",
    "compile = False       # Whether to use PyTorch 2.0 compilation\n",
    "                      # Compilation can speed up training by 20-30%\n",
    "                      # Set to False if you encounter compatibility issues\n",
    "                      # Requires PyTorch 2.0+ and compatible hardware\n",
    "\n",
    "load_pretrained = False  # Whether to load a previously trained model\n",
    "                        # Set to True to continue training from a checkpoint\n",
    "                        # Useful for resuming interrupted training sessions\n",
    "                        # Or for fine-tuning on new data\n",
    "\n",
    "# =============================================================================\n",
    "# CHECKPOINTING PARAMETERS - Model Saving and Loading\n",
    "# =============================================================================\n",
    "# These parameters control how we save and load model states\n",
    "\n",
    "checkpoint_dir = 'models/'  # Directory where model checkpoints are saved\n",
    "                            # Creates a 'models' folder in current directory\n",
    "                            # Checkpoints save model weights, optimizer state, etc.\n",
    "\n",
    "checkpoint_fn = \"latest.pt\"  # Filename for the latest checkpoint\n",
    "                             # Updated every time we save a new checkpoint\n",
    "                             # Contains the most recent model state\n",
    "\n",
    "checkpoint_load_fn = \"latest.pt\"  # Filename for loading a checkpoint\n",
    "                                  # Can be changed to load specific checkpoints\n",
    "                                  # Example: \"llm2.pt\" to load a specific model\n",
    "                                  # Useful for loading pre-trained models\n",
    "\n",
    "# =============================================================================\n",
    "# DATA TYPE AND DEVICE CONFIGURATION\n",
    "# =============================================================================\n",
    "# These parameters control computational precision and hardware usage\n",
    "\n",
    "dtype = torch.bfloat16  # Data type for model computations\n",
    "                        # bfloat16 = 16-bit floating point (faster, less memory)\n",
    "                        # Alternative: torch.float32 (more precise but slower)\n",
    "                        # bfloat16 is optimal for modern GPUs (A100, RTX 30/40 series)\n",
    "\n",
    "# =============================================================================\n",
    "# MODE CONFIGURATION\n",
    "# =============================================================================\n",
    "# Controls whether we're training or just using the model\n",
    "\n",
    "inference = False  # Set to True to only run inference (no training)\n",
    "                   # Useful for testing trained models\n",
    "                   # When False, the model will train normally\n",
    "\n",
    "# =============================================================================\n",
    "# DEVICE SELECTION\n",
    "# =============================================================================\n",
    "# Automatically selects the best available device for computation\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: You will be using: \", device)\n",
    "\n",
    "# CUDA = GPU acceleration (much faster for deep learning)\n",
    "# CPU = fallback for systems without GPU support\n",
    "# Always prefer GPU for training large models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "0Z_omi-4fW0s",
    "outputId": "3f8ecc39-b72d-4825-a78a-2705f66a7210"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOGGING AND MONITORING SETUP - Track Your Training Progress\n",
    "# =============================================================================\n",
    "# This section sets up Weights & Biases (wandb) for monitoring training progress,\n",
    "# visualizing metrics, and comparing different experiments. Wandb is a powerful\n",
    "# tool for machine learning experiment tracking and visualization.\n",
    "\n",
    "# =============================================================================\n",
    "# WANDB CONFIGURATION - Experiment Tracking Setup\n",
    "# =============================================================================\n",
    "# Weights & Biases (wandb) is a popular platform for ML experiment tracking\n",
    "# It provides real-time monitoring, visualization, and collaboration features\n",
    "\n",
    "wandb_log = True  # Enable/disable wandb logging\n",
    "                  # Set to False if you don't want to use wandb\n",
    "                  # Recommended to keep True for better experiment tracking\n",
    "\n",
    "wandb_project = \"test\"  # Project name in wandb dashboard\n",
    "                        # Groups related experiments together\n",
    "                        # Change to something descriptive like \"llm-training\"\n",
    "\n",
    "wandb_run_name = \"test-run\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "# Create unique run names with timestamp\n",
    "# Format: \"test-run_2024_01_15_14_30_25\"\n",
    "# Helps distinguish between different training sessions\n",
    "\n",
    "# =============================================================================\n",
    "# WANDB INITIALIZATION - Start Experiment Tracking\n",
    "# =============================================================================\n",
    "if wandb_log:\n",
    "    import wandb  # Import wandb library for experiment tracking\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "    # Initialize wandb session\n",
    "    # Creates a new experiment run in your wandb dashboard\n",
    "    # Will prompt for API key on first use\n",
    "\n",
    "# =============================================================================\n",
    "# WANDB API KEY SETUP - Authentication Required\n",
    "# =============================================================================\n",
    "# The first time you run this code with wandb_log=True, wandb will ask for an API key\n",
    "# You can get your API key from: https://wandb.ai/settings#api\n",
    "# \n",
    "# Alternative: Click the authorization link that appears when you run this cell\n",
    "# The link will take you to: https://wandb.ai/authorize\n",
    "# This is the quickest way to get authenticated\n",
    "#\n",
    "# What wandb tracks:\n",
    "# - Training and validation loss curves\n",
    "# - Learning rate schedules\n",
    "# - Model parameters and hyperparameters\n",
    "# - System metrics (GPU usage, memory)\n",
    "# - Generated text samples\n",
    "# - Training time and iteration counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPrBqt7NhwnZ",
    "outputId": "85aad864-9cd1-4f74-fa76-63a24988a20f"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET LOADING - Load and Inspect Our Training Data\n",
    "# =============================================================================\n",
    "# This section loads the Wikipedia dataset that will be used to train our language model.\n",
    "# The dataset contains a small portion of English Wikipedia text, which provides\n",
    "# diverse, high-quality text for training our model to understand language patterns.\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD WIKIPEDIA DATASET - Read the Training Text\n",
    "# =============================================================================\n",
    "with open('wiki.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()  # Load the entire Wikipedia text into memory\n",
    "                   # This creates a single string containing all the training data\n",
    "                   # The text is already tokenized and ready for training\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET INSPECTION - Examine the Training Data\n",
    "# =============================================================================\n",
    "print(text[10000:10500])  # Display a sample of the dataset\n",
    "                          # Shows characters 10000-10500 from the text\n",
    "                          # This gives us a preview of what the model will learn from\n",
    "                          # You should see coherent English text from Wikipedia\n",
    "                          # The text contains various topics, writing styles, and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDNcMXo_fals",
    "outputId": "8381a7a5-047a-425e-ece7-d958b18721e5"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOKENIZER SETUP - Convert Text to Numbers\n",
    "# =============================================================================\n",
    "# This section sets up the SentencePiece tokenizer, which converts text into\n",
    "# numerical tokens that our neural network can understand. Tokenization is a\n",
    "# crucial step in natural language processing that bridges human language\n",
    "# and machine learning.\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD SENTENCEPIECE TOKENIZER - Initialize the Tokenizer\n",
    "# =============================================================================\n",
    "# SentencePiece is Google's subword tokenization algorithm\n",
    "# It breaks text into meaningful subword units (not just words or characters)\n",
    "# This allows the model to handle unknown words and reduces vocabulary size\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file='wiki_tokenizer.model')\n",
    "# Load the pre-trained tokenizer model\n",
    "# This model was trained on the same Wikipedia dataset we're using\n",
    "# It knows how to split text into optimal subword units\n",
    "\n",
    "# =============================================================================\n",
    "# TOKENIZER VOCABULARY - Get the Size of Our Token Vocabulary\n",
    "# =============================================================================\n",
    "vocab_size = sp.get_piece_size()  # Get the total number of unique tokens\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")\n",
    "# This tells us how many different tokens our model can work with\n",
    "# Typical vocab sizes: 8K-50K tokens\n",
    "# Our model uses a vocabulary of several thousand tokens\n",
    "\n",
    "# =============================================================================\n",
    "# ENCODING AND DECODING FUNCTIONS - Text ↔ Numbers Conversion\n",
    "# =============================================================================\n",
    "# These functions convert between human-readable text and machine-readable numbers\n",
    "\n",
    "encode = lambda s: sp.Encode(s)  # Convert text string to list of token IDs\n",
    "                                 # Example: \"Hello world\" → [154, 32, 2789]\n",
    "                                 # Each number represents a subword unit\n",
    "\n",
    "decode = lambda l: sp.Decode(l)  # Convert list of token IDs back to text\n",
    "                                 # Example: [154, 32, 2789] → \"Hello world\"\n",
    "                                 # Reconstructs the original text from tokens\n",
    "\n",
    "# =============================================================================\n",
    "# TOKENIZER TESTING - Verify It Works Correctly\n",
    "# =============================================================================\n",
    "print(decode(encode(\"Encoding Decoding functions ready\")))\n",
    "# Test the round-trip: text → tokens → text\n",
    "# Should output: \"Encoding Decoding functions ready\"\n",
    "# This confirms our tokenizer is working properly\n",
    "# If this fails, there's an issue with the tokenizer model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tA2mDSq_fhwC"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET TOKENIZATION - Convert Text to Training Data\n",
    "# =============================================================================\n",
    "# This section converts our raw text into numerical tokens that can be used\n",
    "# for training. Tokenization is computationally expensive, so we save the\n",
    "# result to avoid re-processing the same text multiple times.\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK FOR PRE-TOKENIZED DATA - Avoid Re-processing\n",
    "# =============================================================================\n",
    "if os.path.exists(f\"encoded_data.pt\"):\n",
    "    # Load pre-tokenized data if it already exists\n",
    "    print(\"Loading saved encoded data\")\n",
    "    data = torch.load('encoded_data.pt')  # Load the pre-processed tokens\n",
    "    # This is much faster than re-tokenizing the entire dataset\n",
    "    # The file contains a PyTorch tensor of token IDs\n",
    "else:\n",
    "    # =============================================================================\n",
    "    # TOKENIZE THE DATASET - Convert Text to Numbers\n",
    "    # =============================================================================\n",
    "    print(\"Encoding data\")\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)  # Convert text to token IDs\n",
    "    # encode(text) returns a list of token IDs for the entire text\n",
    "    # torch.tensor() converts it to a PyTorch tensor\n",
    "    # dtype=torch.long ensures the tokens are integers (required for embeddings)\n",
    "    \n",
    "    torch.save(data, 'encoded_data.pt')  # Save the tokenized data\n",
    "    # This creates a file that can be loaded quickly in future runs\n",
    "    # Saves significant time on subsequent training sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4B1cPQGnJM0",
    "outputId": "9b5d5e9d-db9a-4411-ef20-2177f99bf469"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET SPLITTING - Create Training and Validation Sets\n",
    "# =============================================================================\n",
    "# This section splits our tokenized dataset into training and validation sets.\n",
    "# This is essential for proper machine learning - we need separate data to\n",
    "# evaluate our model's performance and prevent overfitting.\n",
    "\n",
    "# =============================================================================\n",
    "# CALCULATE DATASET SIZE - Measure Our Training Data\n",
    "# =============================================================================\n",
    "data_size = len(data)  # Get the total number of tokens in our dataset\n",
    "# This tells us how much text we have available for training\n",
    "# More data generally leads to better model performance\n",
    "\n",
    "# =============================================================================\n",
    "# SPLIT THE DATASET - Create Training and Validation Sets\n",
    "# =============================================================================\n",
    "spl = int(0.9 * data_size)  # Calculate 90% split point\n",
    "# 90% for training, 10% for validation is a standard split\n",
    "# This gives us enough data for training while reserving some for evaluation\n",
    "\n",
    "train_data = data[:spl]  # First 90% of tokens for training\n",
    "# The model learns from this data during training\n",
    "# This is the data used to update the model's weights\n",
    "\n",
    "val_data = data[spl:]  # Last 10% of tokens for validation\n",
    "# This data is used to evaluate the model's performance\n",
    "# The model never sees this data during training\n",
    "# It helps us detect overfitting and measure generalization\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET STATISTICS - Display Data Split Information\n",
    "# =============================================================================\n",
    "print(f'Total data: {data_size/1e6:.2f} Million | Training: {len(train_data)/1e6:.2f} Million | Validation: {len(val_data)/1e6:.2f} Million')\n",
    "# Display the size of each dataset in millions of tokens\n",
    "# This helps us understand how much data we're working with\n",
    "# Typical datasets: millions to billions of tokens\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREVIEW - Examine the Tokenized Data\n",
    "# =============================================================================\n",
    "# data[:30] : shows the first 30 token IDs\n",
    "# This gives us a preview of what the model will see during training\n",
    "# Each number represents a token (word or subword unit)\n",
    "# The model learns to predict the next token given the previous ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EGi6Aevnjtp"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS - Essential Utilities for Training\n",
    "# =============================================================================\n",
    "# This section contains utility functions that are crucial for the training process.\n",
    "# These functions handle data batching, which is essential for efficient training\n",
    "# of neural networks.\n",
    "\n",
    "# =============================================================================\n",
    "# GET_BATCH FUNCTION - Create Training Batches\n",
    "# =============================================================================\n",
    "# This function creates batches of training data for our model.\n",
    "# It randomly samples sequences from the dataset and prepares them for training.\n",
    "# Batching is essential for efficient GPU utilization and stable training.\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Create a batch of training or validation data.\n",
    "    \n",
    "    Args:\n",
    "        split (str): Either \"train\" or \"eval\" to select dataset\n",
    "        \n",
    "    Returns:\n",
    "        x (torch.Tensor): Input sequences of shape (batch_size, context_length)\n",
    "        y (torch.Tensor): Target sequences of shape (batch_size, context_length)\n",
    "    \"\"\"\n",
    "    \n",
    "    # =============================================================================\n",
    "    # SELECT DATASET - Choose Training or Validation Data\n",
    "    # =============================================================================\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    # train_data: Used for training the model (updating weights)\n",
    "    # val_data: Used for evaluation (measuring performance)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # RANDOM SAMPLING - Select Random Starting Positions\n",
    "    # =============================================================================\n",
    "    inds = torch.randint(len(data) - context, (batch_size,))  # (BS)\n",
    "    # Randomly select starting positions for each sequence in the batch\n",
    "    # len(data) - context ensures we don't go out of bounds\n",
    "    # batch_size determines how many sequences we sample\n",
    "    \n",
    "    # =============================================================================\n",
    "    # CREATE INPUT SEQUENCES - Extract Context Windows\n",
    "    # =============================================================================\n",
    "    x = torch.stack([data[i: i+context] for i in inds])  # (BS, SL)\n",
    "    # Extract sequences of length 'context' starting at random positions\n",
    "    # Each sequence is a context window for the model to process\n",
    "    # Shape: (batch_size, context_length)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # CREATE TARGET SEQUENCES - Next Token Prediction\n",
    "    # =============================================================================\n",
    "    y = torch.stack([data[i+1: i+context+1] for i in inds])  # (BS, SL)\n",
    "    # Target sequences are the input sequences shifted by 1 position\n",
    "    # This creates the \"next token\" prediction task\n",
    "    # The model learns to predict y[i] given x[i]\n",
    "    \n",
    "    # =============================================================================\n",
    "    # EXAMPLE OF INPUT-OUTPUT PAIRS\n",
    "    # =============================================================================\n",
    "    # First 10 elements of first batch of inputs and labels:\n",
    "    # x[0][:10] -> tensor([ 664,  278, 4031, 4056, 4065, 4062, 4062, 4051, 13, 13])\n",
    "    # y[0][:10] -> tensor([ 278, 4031, 4056, 4065, 4062, 4062, 4051,   13, 13, 4066])\n",
    "    # Notice how y is x shifted by 1 position - this is the next token prediction task\n",
    "    \n",
    "    # =============================================================================\n",
    "    # MOVE TO DEVICE - Transfer Data to GPU/CPU\n",
    "    # =============================================================================\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    # Move tensors to the appropriate device (GPU or CPU)\n",
    "    # This ensures the data is on the same device as the model\n",
    "    # GPU is much faster for matrix operations\n",
    "    \n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dusI66zcouBq",
    "outputId": "46d934e5-78c5-45a7-ea83-120ac72671ff"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BATCH FUNCTION TESTING - Verify Data Batching Works Correctly\n",
    "# =============================================================================\n",
    "# This section provides a way to test the get_batch function to ensure\n",
    "# it's working properly. Uncomment the lines below to see the batch structure\n",
    "# and verify that the data shapes and content are correct.\n",
    "\n",
    "# =============================================================================\n",
    "# TEST BATCH CREATION - Examine Batch Structure\n",
    "# =============================================================================\n",
    "# Uncomment the following lines to test your get_batch function:\n",
    "# x, y = get_batch(\"train\")\n",
    "# print(f\"x.shape: {x.shape}\")  # Should be (batch_size, context_length)\n",
    "# print(f\"y.shape: {y.shape}\")  # Should be (batch_size, context_length)\n",
    "# print(x[0][:10])  # First 10 tokens of the first sequence in the batch\n",
    "# print(y[0][:10])  # First 10 target tokens (should be x shifted by 1)\n",
    "\n",
    "# =============================================================================\n",
    "# WHAT TO EXPECT - Understanding the Output\n",
    "# =============================================================================\n",
    "# x.shape: (8, 512) - 8 sequences, each 512 tokens long\n",
    "# y.shape: (8, 512) - Same shape as x\n",
    "# x[0][:10]: [664, 278, 4031, 4056, 4065, 4062, 4062, 4051, 13, 13]\n",
    "# y[0][:10]: [278, 4031, 4056, 4065, 4062, 4062, 4051, 13, 13, 4066]\n",
    "# Notice how y is x shifted by 1 position - this is the next token prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19G3Q_RKqVBd"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GPT MODEL ARCHITECTURE - The Heart of Our Language Model\n",
    "# =============================================================================\n",
    "# This section defines our GPT (Generative Pre-trained Transformer) model.\n",
    "# It's a decoder-only transformer architecture that learns to predict the next\n",
    "# token in a sequence, which is the foundation of modern language models.\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SPECIFICATIONS - Understanding Our Architecture\n",
    "# =============================================================================\n",
    "# 19 million parameters with the default configuration\n",
    "# Can be trained with 1 single GPU\n",
    "# With 8 Batch Size, should require 4 GB of GPU Memory\n",
    "# With 128 Batch Size, should require 24 GB of GPU Memory\n",
    "# Adjust Batch Size as needed for less or more memory and training speed\n",
    "# Because of small dataset and model, results will be limited but enough to\n",
    "# demonstrate good improvement during the training and understand all the\n",
    "# main technology involved in building LLMs\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT (Generative Pre-trained Transformer) Model\n",
    "    \n",
    "    This is a decoder-only transformer architecture that learns to predict\n",
    "    the next token in a sequence. It consists of:\n",
    "    - Token embeddings\n",
    "    - Position embeddings  \n",
    "    - Multiple transformer blocks\n",
    "    - Layer normalization\n",
    "    - Final linear layer for vocabulary prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # EMBEDDING LAYERS - Convert Tokens to Rich Representations\n",
    "        # =============================================================================\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        # Token embeddings: Convert token IDs to dense vectors\n",
    "        # Shape: (vocab_size, embed_size) - lookup table for each token\n",
    "        # Each token gets a unique dense representation\n",
    "        \n",
    "        self.positions = nn.Embedding(context, embed_size)\n",
    "        # Position embeddings: Encode position information\n",
    "        # Shape: (context, embed_size) - one embedding per position\n",
    "        # Allows the model to understand word order and position\n",
    "        \n",
    "        # =============================================================================\n",
    "        # TRANSFORMER BLOCKS - The Core Processing Units\n",
    "        # =============================================================================\n",
    "        self.blocks = nn.Sequential(*[Block(n_heads) for _ in range(n_layers)])\n",
    "        # Stack multiple transformer blocks\n",
    "        # Each block contains multi-head attention and feed-forward layers\n",
    "        # More blocks = deeper model = more complex reasoning\n",
    "        \n",
    "        # =============================================================================\n",
    "        # FINAL LAYERS - Output Processing\n",
    "        # =============================================================================\n",
    "        self.ln = nn.LayerNorm(embed_size)  # Final layer normalization\n",
    "        # Normalizes the output before the final linear layer\n",
    "        # Helps with training stability and performance\n",
    "        \n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS)\n",
    "        # Final linear layer: Maps from embedding space to vocabulary\n",
    "        # Outputs logits for each token in the vocabulary\n",
    "        # Shape: (batch_size, sequence_length, vocab_size)\n",
    "        \n",
    "        # =============================================================================\n",
    "        # WEIGHT INITIALIZATION - Start with Good Weights\n",
    "        # =============================================================================\n",
    "        self.apply(self._init_weights)  # Initialize all weights\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initialize model weights with appropriate distributions.\n",
    "        Good initialization is crucial for training stability.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Initialize weight matrices with normal distribution\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            # Small standard deviation prevents initial activations from being too large\n",
    "            # Mean 0 centers the distribution around zero\n",
    "            \n",
    "            # Initialize bias parameters to 0\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                # Zero bias is a good starting point for most layers\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Initialize embedding weights with normal distribution\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            # Same initialization as linear layers for consistency\n",
    "\n",
    "    def forward(self, input, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the GPT model.\n",
    "        \n",
    "        Args:\n",
    "            input (torch.Tensor): Input token IDs of shape (batch_size, sequence_length)\n",
    "            targets (torch.Tensor, optional): Target token IDs for training\n",
    "            \n",
    "        Returns:\n",
    "            logits (torch.Tensor): Predicted logits for each token\n",
    "            loss (torch.Tensor, optional): Cross-entropy loss if targets provided\n",
    "        \"\"\"\n",
    "        \n",
    "        # =============================================================================\n",
    "        # INPUT PROCESSING - Convert Tokens to Embeddings\n",
    "        # =============================================================================\n",
    "        BS, SL = input.shape  # Batch Size, Sequence Length\n",
    "        # Example: (8, 512) for batch_size=8, context=512\n",
    "        \n",
    "        emb = self.embeddings(input)  # (BS, SL, embed_size)\n",
    "        # Convert token IDs to dense embeddings\n",
    "        # Each token becomes a vector of size embed_size\n",
    "        \n",
    "        pos = self.positions(torch.arange(SL, device=device))  # (SL, embed_size)\n",
    "        # Get position embeddings for each position in the sequence\n",
    "        # Position 0 gets pos[0], position 1 gets pos[1], etc.\n",
    "        \n",
    "        x = emb + pos  # (BS, SL, embed_size)\n",
    "        # Add token and position embeddings\n",
    "        # This gives each token both semantic and positional information\n",
    "        \n",
    "        # =============================================================================\n",
    "        # TRANSFORMER PROCESSING - Apply Multiple Blocks\n",
    "        # =============================================================================\n",
    "        x = self.blocks(x)  # (BS, SL, embed_size)\n",
    "        # Pass through all transformer blocks\n",
    "        # Each block applies multi-head attention and feed-forward layers\n",
    "        # The model learns complex relationships between tokens\n",
    "        \n",
    "        x = self.ln(x)  # (BS, SL, embed_size)\n",
    "        # Final layer normalization\n",
    "        # Normalizes the output before the final linear layer\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OUTPUT GENERATION - Predict Next Tokens\n",
    "        # =============================================================================\n",
    "        logits = self.final_linear(x)  # (BS, SL, vocab_size)\n",
    "        # Convert embeddings to vocabulary logits\n",
    "        # Each position gets a probability distribution over all tokens\n",
    "        \n",
    "        # =============================================================================\n",
    "        # LOSS CALCULATION - Cross-Entropy Loss for Training\n",
    "        # =============================================================================\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Reshape for cross-entropy loss calculation\n",
    "            BS, SL, VS = logits.shape  # (BS, SL, vocab_size)\n",
    "            logits = logits.view(BS * SL, VS)  # (BS*SL, vocab_size)\n",
    "            targets = targets.view(BS * SL)    # (BS*SL,)\n",
    "            \n",
    "            # Calculate cross-entropy loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            # Cross-entropy measures how well our predictions match the targets\n",
    "            # Lower loss = better predictions\n",
    "            \n",
    "            # =============================================================================\n",
    "            # MANUAL LOSS CALCULATION (Optional - for educational purposes)\n",
    "            # =============================================================================\n",
    "            # Uncomment the following lines to see manual cross-entropy calculation:\n",
    "            # counts = logits.exp()  # (BS*SL, vocab_size)\n",
    "            # prob = counts / counts.sum(-1, keepdim=True)  # (BS*SL, vocab_size)\n",
    "            # loss2 = -prob[torch.arange(BS*SL), targets].log().mean()\n",
    "            # \n",
    "            # This shows the mathematical steps behind cross-entropy:\n",
    "            # 1. Apply softmax to get probabilities\n",
    "            # 2. Select probabilities for target tokens\n",
    "            # 3. Take negative log (higher probability = lower loss)\n",
    "            # 4. Take mean across all predictions\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input, max=500):\n",
    "        \"\"\"\n",
    "        Generate new text by sampling from the model.\n",
    "        \n",
    "        Args:\n",
    "            input (torch.Tensor): Starting sequence of token IDs\n",
    "            max (int): Maximum number of tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Generated sequence of token IDs\n",
    "        \"\"\"\n",
    "        \n",
    "        # =============================================================================\n",
    "        # TEXT GENERATION - Autoregressive Sampling\n",
    "        # =============================================================================\n",
    "        for _ in range(max):  # Generate up to 'max' tokens\n",
    "            # Keep only the last 'context' tokens (sliding window)\n",
    "            input = input[:, -context:]  # (1, context)\n",
    "            # This ensures we don't exceed the model's context length\n",
    "            \n",
    "            # Get predictions from the model\n",
    "            logits, _ = self(input)  # (1, context, vocab_size)\n",
    "            # Get logits for the entire sequence\n",
    "            \n",
    "            # Focus on the last position (next token prediction)\n",
    "            logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "            # We only care about predicting the next token\n",
    "            \n",
    "            # Convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (1, vocab_size)\n",
    "            # Softmax converts logits to probability distribution\n",
    "            \n",
    "            # Sample next token from the distribution\n",
    "            next = torch.multinomial(probs, num_samples=1)  # (1, 1)\n",
    "            # Multinomial sampling: higher probability tokens are more likely\n",
    "            # This adds randomness to generation (not just greedy selection)\n",
    "            \n",
    "            # Append the new token to the sequence\n",
    "            input = torch.cat((input, next), dim=1)  # (1, context+1)\n",
    "            # The sequence grows by one token each iteration\n",
    "            \n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXdTAGEWp-nz"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRANSFORMER BLOCK CLASS - The Building Blocks of Our Model\n",
    "# =============================================================================\n",
    "# This section defines the Transformer Block, which is the core component\n",
    "# of our GPT model. Each block combines attention mechanisms with feed-forward\n",
    "# networks to process and understand relationships in the input sequence.\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Block - The Core Processing Unit\n",
    "    \n",
    "    A transformer block combines communication (attention) and computation (feed-forward)\n",
    "    to process sequences. It helps the model understand relationships between tokens\n",
    "    and perform complex reasoning through attention mechanisms.\n",
    "    \n",
    "    Architecture:\n",
    "    - Multi-head attention (communication between tokens)\n",
    "    - Feed-forward network (computation and transformation)\n",
    "    - Layer normalization (stability and performance)\n",
    "    - Residual connections (gradient flow and training stability)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # ATTENTION MECHANISM - Multi-Head Attention Setup\n",
    "        # =============================================================================\n",
    "        head_size = embed_size // n_heads  # Calculate size of each attention head\n",
    "        # We split the embedding dimensions among the number of heads\n",
    "        # Example: embed_size=384, n_heads=7 → head_size=54 (approximately)\n",
    "        # Each head processes a subset of the embedding dimensions\n",
    "        \n",
    "        self.ma = Multihead(n_heads, head_size)  # Multi-head attention layer\n",
    "        # This is the core attention mechanism that allows tokens to \"attend\" to each other\n",
    "        # Multiple heads allow the model to focus on different types of relationships\n",
    "        \n",
    "        # =============================================================================\n",
    "        # FEED-FORWARD NETWORK - Computation and Transformation\n",
    "        # =============================================================================\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        # Feed-forward network that processes each token independently\n",
    "        # Applies non-linear transformations to the attention output\n",
    "        # Increases the model's capacity for complex reasoning\n",
    "        \n",
    "        # =============================================================================\n",
    "        # LAYER NORMALIZATION - Training Stability\n",
    "        # =============================================================================\n",
    "        self.ln1 = nn.LayerNorm(embed_size)  # Normalization before attention\n",
    "        self.ln2 = nn.LayerNorm(embed_size)  # Normalization before feed-forward\n",
    "        \n",
    "        # LayerNorm normalizes the inputs across the features for each data point independently\n",
    "        # It subtracts the mean and divides by the standard deviation, followed by scaling and shifting\n",
    "        # This helps with training stability and allows for higher learning rates\n",
    "        # More computationally intensive than RMSNorm but offers greater flexibility\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input embeddings of shape (batch_size, sequence_length, embed_size)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Processed embeddings of the same shape\n",
    "        \"\"\"\n",
    "        \n",
    "        # =============================================================================\n",
    "        # ATTENTION PATH - Communication Between Tokens\n",
    "        # =============================================================================\n",
    "        x = x + self.ma(self.ln1(x))  # Residual connection around attention\n",
    "        # 1. Normalize the input (ln1)\n",
    "        # 2. Apply multi-head attention (ma)\n",
    "        # 3. Add residual connection (x + ...)\n",
    "        # The residual connection helps with gradient flow and training stability\n",
    "        \n",
    "        # =============================================================================\n",
    "        # FEED-FORWARD PATH - Computation and Transformation\n",
    "        # =============================================================================\n",
    "        x = x + self.feed_forward(self.ln2(x))  # Residual connection around feed-forward\n",
    "        # 1. Normalize the attention output (ln2)\n",
    "        # 2. Apply feed-forward network\n",
    "        # 3. Add residual connection (x + ...)\n",
    "        # This allows the model to learn complex transformations\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPT2Akr9tB-c"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FORWARD LAYER - Feed-Forward Network for Complex Processing\n",
    "# =============================================================================\n",
    "# This section defines the feed-forward network that processes each token\n",
    "# independently. It increases the computational complexity and allows the\n",
    "# model to learn complex transformations and patterns.\n",
    "\n",
    "class ForwardLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-Forward Network Layer\n",
    "    \n",
    "    The ForwardLayer applies a network that increases the computational complexity\n",
    "    of the processing. It processes each token independently and applies non-linear\n",
    "    transformations to the attention output.\n",
    "    \n",
    "    Architecture:\n",
    "    - Linear layer: embed_size → 6*embed_size (expansion)\n",
    "    - GELU activation: Non-linear transformation\n",
    "    - Linear layer: 6*embed_size → embed_size (projection)\n",
    "    - Dropout: Regularization to prevent overfitting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # FEED-FORWARD NETWORK ARCHITECTURE - Multi-Layer Processing\n",
    "        # =============================================================================\n",
    "        self.network = nn.Sequential(\n",
    "            # First linear layer: Expand the embedding dimension\n",
    "            nn.Linear(embed_size, 6*embed_size, bias=BIAS),\n",
    "            # Expands from embed_size to 6*embed_size (e.g., 384 → 2304)\n",
    "            # This gives the network more capacity for complex transformations\n",
    "            # The 6x expansion is a common choice in transformer architectures\n",
    "            \n",
    "            # GELU activation function\n",
    "            nn.GELU(),\n",
    "            # Gaussian Error Linear Unit - smooth, non-linear activation\n",
    "            # GELU(x) = x * Φ(x) where Φ is the standard normal CDF\n",
    "            # More smooth than ReLU, often performs better in transformers\n",
    "            # Allows for negative values, which can be beneficial\n",
    "            \n",
    "            # Second linear layer: Project back to original dimension\n",
    "            nn.Linear(6*embed_size, embed_size, bias=BIAS),\n",
    "            # Projects back from 6*embed_size to embed_size (e.g., 2304 → 384)\n",
    "            # This maintains the same output dimension as input\n",
    "            # The network learns to compress the expanded representation\n",
    "            \n",
    "            # Dropout for regularization\n",
    "            nn.Dropout(dropout)\n",
    "            # Randomly sets some neurons to zero during training\n",
    "            # Prevents overfitting and improves generalization\n",
    "            # Only active during training, not during inference\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input embeddings of shape (batch_size, sequence_length, embed_size)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Processed embeddings of the same shape\n",
    "        \"\"\"\n",
    "        x = self.network(x)  # Apply the entire feed-forward network\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNdQG5IotEtj"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTI-HEAD ATTENTION LAYER - Parallel Attention Processing\n",
    "# =============================================================================\n",
    "# This section defines the Multi-Head Attention mechanism, which is the core\n",
    "# of the transformer architecture. It allows the model to attend to different\n",
    "# parts of the sequence simultaneously and learn complex relationships.\n",
    "\n",
    "class Multihead(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention Layer\n",
    "    \n",
    "    This layer coordinates multiple attention heads within each transformer block.\n",
    "    Each head can focus on different types of relationships, allowing the model\n",
    "    to capture various patterns and dependencies in the sequence.\n",
    "    \n",
    "    Architecture:\n",
    "    - Multiple attention heads (parallel processing)\n",
    "    - Linear combination of head outputs\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # ATTENTION HEADS - Parallel Processing Units\n",
    "        # =============================================================================\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        # Setup multiple attention heads\n",
    "        # head_size = embed_size // n_heads (e.g., 384 // 7 ≈ 54)\n",
    "        # Each head processes a subset of the embedding dimensions\n",
    "        # Multiple heads allow the model to focus on different types of relationships\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OUTPUT COMBINATION - Merge Head Results\n",
    "        # =============================================================================\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias=BIAS)\n",
    "        # Linear layer to combine outputs from all heads\n",
    "        # Input: head_size * n_heads (e.g., 54 * 7 = 378)\n",
    "        # Output: embed_size (e.g., 384)\n",
    "        # Projects the concatenated head outputs back to the original embedding size\n",
    "        \n",
    "        # =============================================================================\n",
    "        # REGULARIZATION - Prevent Overfitting\n",
    "        # =============================================================================\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Dropout applied to the final output\n",
    "        # Helps prevent overfitting during training\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input embeddings of shape (batch_size, sequence_length, embed_size)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Attention output of shape (batch_size, sequence_length, embed_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # =============================================================================\n",
    "        # PARALLEL ATTENTION PROCESSING - Multiple Heads Working Together\n",
    "        # =============================================================================\n",
    "        # x is (BS, SL, embed_size)  # e.g., (8, 512, 384)\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # Each head processes the input independently\n",
    "        # Each head outputs (BS, SL, head_size)\n",
    "        # Concatenating all heads produces (BS, SL, head_size * n_heads)\n",
    "        # Example: (8, 512, 378) where 378 = 54 * 7 heads\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OUTPUT PROJECTION - Combine and Transform\n",
    "        # =============================================================================\n",
    "        x = self.combine(x)  # Project back to embed_size (BS, SL, embed_size)\n",
    "        # Linear transformation to combine all head outputs\n",
    "        # Projects from (head_size * n_heads) back to embed_size\n",
    "        # This allows the model to learn how to combine information from different heads\n",
    "        \n",
    "        # =============================================================================\n",
    "        # REGULARIZATION - Apply Dropout\n",
    "        # =============================================================================\n",
    "        x = self.dropout(x)  # Apply dropout for regularization\n",
    "        # Randomly sets some outputs to zero during training\n",
    "        # Helps prevent overfitting and improves generalization\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34nHc2eJtH17"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ATTENTION HEAD - The Core of Self-Attention Mechanism\n",
    "# =============================================================================\n",
    "# This section defines the individual attention head, which is the fundamental\n",
    "# building block of the transformer architecture. It implements the self-attention\n",
    "# mechanism that allows tokens to attend to each other and learn relationships.\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Attention Head - Self-Attention Mechanism\n",
    "    \n",
    "    Detects and reinforces patterns in relationships between members of sequence.\n",
    "    Each head implements the core self-attention mechanism with Query, Key, and Value\n",
    "    projections, allowing tokens to attend to each other and learn dependencies.\n",
    "    \n",
    "    Architecture:\n",
    "    - Query, Key, Value projections\n",
    "    - Scaled dot-product attention\n",
    "    - Causal masking (prevents looking at future tokens)\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # QUERY, KEY, VALUE PROJECTIONS - The Foundation of Attention\n",
    "        # =============================================================================\n",
    "        self.queries = nn.Linear(embed_size, head_size, bias=BIAS)  # Query Projection\n",
    "        # Projects input embeddings to query vectors\n",
    "        # Shape: (embed_size, head_size) e.g., (384, 54)\n",
    "        # Queries represent \"what am I looking for?\"\n",
    "        \n",
    "        self.keys = nn.Linear(embed_size, head_size, bias=BIAS)  # Key Projection\n",
    "        # Projects input embeddings to key vectors\n",
    "        # Shape: (embed_size, head_size) e.g., (384, 54)\n",
    "        # Keys represent \"what can I provide?\"\n",
    "        \n",
    "        self.values = nn.Linear(embed_size, head_size, bias=BIAS)  # Value Projection\n",
    "        # Projects input embeddings to value vectors\n",
    "        # Shape: (embed_size, head_size) e.g., (384, 54)\n",
    "        # Values represent \"what information do I contain?\"\n",
    "        \n",
    "        # =============================================================================\n",
    "        # CAUSAL MASKING - Prevent Looking at Future Tokens\n",
    "        # =============================================================================\n",
    "        # We declare a triangular matrix that we will use to mask future tokens\n",
    "        # self.tril contains 0s in upper triangle and 1s in lower triangle + diagonal\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context, context)))\n",
    "        # Shape: (context, context) e.g., (512, 512)\n",
    "        # Lower triangular matrix: 1s below diagonal, 0s above\n",
    "        # This ensures each token can only attend to previous tokens (causal attention)\n",
    "        # Essential for autoregressive language modeling\n",
    "        \n",
    "        # =============================================================================\n",
    "        # REGULARIZATION - Prevent Overfitting\n",
    "        # =============================================================================\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Dropout applied to attention weights\n",
    "        # Helps prevent overfitting during training\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the attention head.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input embeddings of shape (batch_size, sequence_length, embed_size)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Attention output of shape (batch_size, sequence_length, head_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        BS, SL, VS = x.shape  # Batch Size, Sequence Length, Embedding Size\n",
    "        \n",
    "        # =============================================================================\n",
    "        # QUERY, KEY, VALUE COMPUTATION - Project Input Embeddings\n",
    "        # =============================================================================\n",
    "        q = self.queries(x)  # (BS, SL, head_size) e.g., (8, 512, 54)\n",
    "        # Compute query vectors for each token\n",
    "        # Each token gets a query vector representing what it's looking for\n",
    "        \n",
    "        k = self.keys(x)  # (BS, SL, head_size) e.g., (8, 512, 54)\n",
    "        # Compute key vectors for each token\n",
    "        # Each token gets a key vector representing what it can provide\n",
    "        \n",
    "        v = self.values(x)  # (BS, SL, head_size) e.g., (8, 512, 54)\n",
    "        # Compute value vectors for each token\n",
    "        # Each token gets a value vector containing its information\n",
    "        \n",
    "        # =============================================================================\n",
    "        # ATTENTION WEIGHTS COMPUTATION - Scaled Dot-Product Attention\n",
    "        # =============================================================================\n",
    "        # Calculate attention weights matrix with dot product of q and k, and normalize\n",
    "        attn_w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5  # (BS, SL, SL)\n",
    "        # q @ k.transpose(-2, -1): Dot product between queries and keys\n",
    "        # Shape: (BS, SL, SL) - attention weights for each token pair\n",
    "        # k.shape[-1]**-0.5: Scaling factor (1/√head_size) for stability\n",
    "        # This prevents attention weights from becoming too large\n",
    "        \n",
    "        # =============================================================================\n",
    "        # CAUSAL MASKING - Apply Causal Attention\n",
    "        # =============================================================================\n",
    "        # Mask out future tokens, pay attention only to the past\n",
    "        attn_w = attn_w.masked_fill(self.tril[:SL, :SL] == 0, float('-inf'))\n",
    "        # Set attention weights to -inf for future tokens (upper triangle)\n",
    "        # This ensures each token can only attend to previous tokens\n",
    "        # Essential for autoregressive generation\n",
    "        \n",
    "        # =============================================================================\n",
    "        # SOFTMAX NORMALIZATION - Convert to Probabilities\n",
    "        # =============================================================================\n",
    "        attn_w = F.softmax(attn_w, dim=-1)  # Transform into probabilities (BS, SL, SL)\n",
    "        # Apply softmax to get attention probabilities\n",
    "        # Each row sums to 1 (probability distribution over all tokens)\n",
    "        # Higher values indicate stronger attention\n",
    "        \n",
    "        # =============================================================================\n",
    "        # DROPOUT REGULARIZATION - Prevent Overfitting\n",
    "        # =============================================================================\n",
    "        attn_w = self.dropout(attn_w)  # (BS, SL, SL)\n",
    "        # Apply dropout to attention weights\n",
    "        # Randomly sets some attention weights to zero\n",
    "        # Helps prevent overfitting and improves generalization\n",
    "        \n",
    "        # =============================================================================\n",
    "        # ATTENTION OUTPUT - Weighted Combination of Values\n",
    "        # =============================================================================\n",
    "        # Use attention weights to update the features of our tokens\n",
    "        x = attn_w @ v  # (BS, SL, head_size)\n",
    "        # Matrix multiplication: attention weights × values\n",
    "        # Each token gets a weighted combination of all value vectors\n",
    "        # The attention weights determine how much each token contributes\n",
    "        # This is the final output of the attention mechanism\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKRu7PKctLIS",
    "outputId": "2b6861cf-fc98-4a3b-c91c-8233cb613d79"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL INSTANTIATION AND SETUP - Create and Configure Our LLM\n",
    "# =============================================================================\n",
    "# This section creates our GPT model instance and configures it for training.\n",
    "# We set up the model with the right data types, device placement, and\n",
    "# optional compilation for optimal performance.\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL CREATION - Instantiate Our GPT Model\n",
    "# =============================================================================\n",
    "model = GPT()  # Create a new GPT model instance\n",
    "# This initializes all the layers we defined:\n",
    "# - Token and position embeddings\n",
    "# - Multiple transformer blocks\n",
    "# - Final linear layer\n",
    "# - All with proper weight initialization\n",
    "\n",
    "# =============================================================================\n",
    "# DATA TYPE CONFIGURATION - Set Precision for Training\n",
    "# =============================================================================\n",
    "model = model.to(dtype)  # Set the precision type\n",
    "# Convert model to the specified data type (e.g., bfloat16)\n",
    "# This affects memory usage and training speed\n",
    "# bfloat16 uses less memory and can be faster on modern GPUs\n",
    "\n",
    "# =============================================================================\n",
    "# DEVICE PLACEMENT - Move Model to GPU/CPU\n",
    "# =============================================================================\n",
    "model = model.to(device)  # Move model to the appropriate device\n",
    "# Move all model parameters and buffers to GPU or CPU\n",
    "# GPU is much faster for training deep neural networks\n",
    "# All computations will now happen on the specified device\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPILATION - Optional Performance Optimization\n",
    "# =============================================================================\n",
    "# Torch.compile compiles a PyTorch model to an optimized version,\n",
    "# aiming to improve runtime performance and efficiency.\n",
    "# This can provide 20-30% speedup on compatible systems\n",
    "if compile:\n",
    "    print(\"Torch :: Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "    # Compiles the model for faster execution\n",
    "    # Requires PyTorch 2.0+ and compatible hardware\n",
    "    # May take some time on first run but speeds up subsequent runs\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL PARAMETER COUNT - Display Model Size\n",
    "# =============================================================================\n",
    "# Print the number of parameters of our model (19 million in our case)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \" Million parameters\")\n",
    "# Count total number of trainable parameters\n",
    "# numel() returns the number of elements in each parameter tensor\n",
    "# Sum all parameters and convert to millions\n",
    "# This gives us an idea of model complexity and memory requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvX0LI8HtR_h"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOSS CALCULATION - Evaluate Model Performance\n",
    "# =============================================================================\n",
    "# This section defines a function to calculate the model's loss on both\n",
    "# training and validation data. This is essential for monitoring training\n",
    "# progress and detecting overfitting.\n",
    "\n",
    "@torch.no_grad()  # Prevent gradient calculation during evaluation\n",
    "def calculate_loss():\n",
    "    \"\"\"\n",
    "    Calculate the model's loss on training and validation data.\n",
    "    \n",
    "    This function evaluates the model's performance without updating weights.\n",
    "    It's used to monitor training progress and detect overfitting.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing 'train' and 'eval' loss values\n",
    "    \"\"\"\n",
    "    \n",
    "    out = {}  # Dictionary to store results\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # =============================================================================\n",
    "    # EVALUATION LOOP - Test on Both Training and Validation Data\n",
    "    # =============================================================================\n",
    "    for split in ['train', 'eval']:\n",
    "        # Create tensor to store loss values\n",
    "        l = torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        \n",
    "        # =============================================================================\n",
    "        # BATCH EVALUATION - Calculate Loss on Multiple Batches\n",
    "        # =============================================================================\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split)  # Get a new batch of data\n",
    "            _, loss = model(x, y)  # Calculate the loss\n",
    "            l[i] = loss  # Store the loss in the next position of tensor\n",
    "            \n",
    "        # =============================================================================\n",
    "        # LOSS AVERAGING - Get Mean Loss Across Batches\n",
    "        # =============================================================================\n",
    "        out[split] = l.mean().item()  # Calculate the mean and extract the final value\n",
    "        # Average the loss across all evaluation batches\n",
    "        # This gives us a more stable estimate of model performance\n",
    "        \n",
    "    # =============================================================================\n",
    "    # RESTORE TRAINING MODE - Switch Back to Training\n",
    "    # =============================================================================\n",
    "    model.train()  # Set model back to training mode\n",
    "    # This ensures dropout and other training-specific behaviors are restored\n",
    "    \n",
    "    return out\n",
    "\n",
    "# =============================================================================\n",
    "# INITIAL LOSS CALCULATION - Baseline Performance\n",
    "# =============================================================================\n",
    "l = calculate_loss()  # Calculate initial loss\n",
    "print(l)  # Display the results\n",
    "# This shows us the model's performance before training\n",
    "# Training loss should be high initially (random predictions)\n",
    "# Validation loss should be similar to training loss (no overfitting yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X20ZTtOu9mJ",
    "outputId": "bcf6bf71-bf8a-4412-c9b0-283d528943d7"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEXT GENERATION - Create New Text with Our Model\n",
    "# =============================================================================\n",
    "# This section defines a function to generate new text using our trained model.\n",
    "# It demonstrates the model's ability to create coherent text by sampling\n",
    "# from the learned probability distributions.\n",
    "\n",
    "@torch.no_grad()  # Disable gradient calculation during generation\n",
    "def generate_sample(input):\n",
    "    \"\"\"\n",
    "    Generate new text using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        input (str): Starting text prompt for generation\n",
    "    \"\"\"\n",
    "    \n",
    "    # =============================================================================\n",
    "    # INPUT TOKENIZATION - Convert Text to Numbers\n",
    "    # =============================================================================\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device)\n",
    "    # Tokenize the input string into token IDs\n",
    "    # encode(input): Convert text to list of token IDs\n",
    "    # torch.tensor(): Convert to PyTorch tensor\n",
    "    # dtype=torch.long: Ensure integer type for token IDs\n",
    "    # device=device: Move to the same device as the model\n",
    "    \n",
    "    t1 = t1[None, :]  # (1, [size of ids])\n",
    "    # Add batch dimension: (sequence_length,) → (1, sequence_length)\n",
    "    # The model expects batched input\n",
    "    \n",
    "    # =============================================================================\n",
    "    # TEXT GENERATION - Sample from the Model\n",
    "    # =============================================================================\n",
    "    newgen = model.generate(t1, max=64)[0].tolist()\n",
    "    # model.generate(): Generate new tokens\n",
    "    # max=64: Limit generation to 64 tokens\n",
    "    # [0]: Get the first (and only) sequence from the batch\n",
    "    # .tolist(): Convert tensor to Python list\n",
    "    \n",
    "    # =============================================================================\n",
    "    # OUTPUT DECODING - Convert Numbers Back to Text\n",
    "    # =============================================================================\n",
    "    result = decode(newgen)  # Decode token IDs back to text\n",
    "    # decode(): Convert list of token IDs to human-readable text\n",
    "    # This gives us the final generated text\n",
    "    \n",
    "    print(f\"{result}\")  # Display the generated text\n",
    "\n",
    "# =============================================================================\n",
    "# SAMPLE GENERATION - Test the Model\n",
    "# =============================================================================\n",
    "generate_sample(\"The mountain in my city is\")  # Generate a sample\n",
    "# This demonstrates the model's text generation capabilities\n",
    "# The model will try to complete the sentence based on its training\n",
    "# Initially, the output may be random or incoherent\n",
    "# As training progresses, the output should become more coherent and relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z9sOljjvq2l"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTIMIZER AND SCHEDULER SETUP - Configure Training Optimization\n",
    "# =============================================================================\n",
    "# This section sets up the optimizer and learning rate scheduler for training.\n",
    "# We use different weight decay settings for different parameter types to\n",
    "# optimize training performance and prevent overfitting.\n",
    "\n",
    "# =============================================================================\n",
    "# PARAMETER GROUPING - Different Treatment for Different Parameters\n",
    "# =============================================================================\n",
    "# Set Weight Decay differently for different kinds of parameters\n",
    "# parameter dictionary where keys are parameter names, and values are the parameter themselves\n",
    "p_dict = {p_name: p for p_name, p in model.named_parameters() if p.requires_grad}\n",
    "# Get all trainable parameters from the model\n",
    "# len: 370 (total number of parameter tensors)\n",
    "\n",
    "# =============================================================================\n",
    "# WEIGHT MATRICES - Parameters that Benefit from Weight Decay\n",
    "# =============================================================================\n",
    "# Isolate weight matrices as they benefit specially from weight decay\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]  # len: 171\n",
    "# Parameters with 2 or more dimensions (weight matrices)\n",
    "# These include linear layer weights, embedding weights, etc.\n",
    "# Weight decay helps prevent these from becoming too large\n",
    "\n",
    "# =============================================================================\n",
    "# BIAS AND OTHER PARAMETERS - Parameters that Don't Need Weight Decay\n",
    "# =============================================================================\n",
    "# Isolate other parameters like bias parameters, that don't benefit from weight decay\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2]  # len: 199\n",
    "# Parameters with less than 2 dimensions (bias terms, scalars)\n",
    "# These don't benefit from weight decay and can be left unregularized\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZER GROUPS - Different Settings for Different Parameters\n",
    "# =============================================================================\n",
    "# Store the parameter types in a list of dictionaries\n",
    "optimizer_groups = [\n",
    "    {'params': weight_decay_p, 'weight_decay': weight_decay},  # Apply weight decay\n",
    "    {'params': no_weight_decay_p, 'weight_decay': 0.0}        # No weight decay\n",
    "]\n",
    "# This allows us to apply different regularization to different parameter types\n",
    "\n",
    "# =============================================================================\n",
    "# ADAMW OPTIMIZER - Advanced Gradient Descent\n",
    "# =============================================================================\n",
    "# Declare optimizer, it helps us compute gradients, update parameters, manage learning rate, apply weight decay\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, lr=lr, betas=(0.9, 0.99))\n",
    "# AdamW: Adam optimizer with decoupled weight decay\n",
    "# lr: Learning rate for parameter updates\n",
    "# betas: Control the exponential moving averages of the gradient and its square\n",
    "# (0.9, 0.99): Standard values for momentum and squared gradient momentum\n",
    "# These are essential components of the Adam and AdamW optimization algorithms\n",
    "\n",
    "# =============================================================================\n",
    "# LEARNING RATE SCHEDULER - Dynamic Learning Rate\n",
    "# =============================================================================\n",
    "# Declare scheduler to change learning rate through the training\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_iters, eta_min=lr/10)\n",
    "# CosineAnnealingLR: Cosine annealing learning rate schedule\n",
    "# train_iters: Total number of training iterations\n",
    "# eta_min: Minimum learning rate (lr/10)\n",
    "# Learning rate will descend from lr to lr/10 following a cosine curve\n",
    "# This helps with convergence and can improve final performance\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING STATE INITIALIZATION - Track Training Progress\n",
    "# =============================================================================\n",
    "start_iteration = 0  # Starting iteration number\n",
    "best_val_loss = float('inf')  # Track best validation loss value\n",
    "# Initialize with infinity so any improvement will be recorded\n",
    "# This helps us save the best model during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXt7xGMvwQ_5"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHECKPOINT LOADING - Resume Training from Saved State\n",
    "# =============================================================================\n",
    "# This section provides functionality to load previously saved model checkpoints.\n",
    "# This allows us to resume training from where we left off or load a\n",
    "# pre-trained model for further training.\n",
    "\n",
    "# =============================================================================\n",
    "# CHECKPOINT LOADING FUNCTION - Restore Model State\n",
    "# =============================================================================\n",
    "def load_checkpoint(path):\n",
    "    \"\"\"\n",
    "    Load a previously saved checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the checkpoint file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (iteration, loss) - The iteration and loss from the checkpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"LLM - Loading model\")\n",
    "    checkpoint = torch.load(path)  # Load the checkpoint file\n",
    "    \n",
    "    # =============================================================================\n",
    "    # MODEL STATE RESTORATION - Load Model Parameters\n",
    "    # =============================================================================\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  # Load model parameters\n",
    "    # Restore all model weights and biases to their saved values\n",
    "    # This brings the model back to the exact state when it was saved\n",
    "    \n",
    "    # =============================================================================\n",
    "    # OPTIMIZER STATE RESTORATION - Load Optimizer State\n",
    "    # =============================================================================\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # Load optimizer state\n",
    "    # Restore optimizer internal state (momentum, learning rate, etc.)\n",
    "    # This ensures training continues smoothly from where it left off\n",
    "    \n",
    "    # =============================================================================\n",
    "    # TRAINING STATE RESTORATION - Load Training Progress\n",
    "    # =============================================================================\n",
    "    iteration = checkpoint['iteration']  # In what iteration did we save the model?\n",
    "    loss = checkpoint['loss']  # What was the last loss value?\n",
    "    \n",
    "    print(f\"Loaded iter {iteration} with loss {loss}\")\n",
    "    return iteration, loss\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIONAL CHECKPOINT LOADING - Resume from Previous Training\n",
    "# =============================================================================\n",
    "# Load a previous checkpoint if it exists and load_pretrained is True\n",
    "if os.path.exists(f\"{checkpoint_dir}/{checkpoint_load_fn}\") and load_pretrained:\n",
    "    start_iteration, loss = load_checkpoint(checkpoint_dir + checkpoint_load_fn)\n",
    "    best_val_loss = loss\n",
    "    # Update the starting iteration and best validation loss\n",
    "    # This allows us to continue training from where we left off\n",
    "    # Useful for resuming interrupted training sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2KPyh1cwo3t"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INFERENCE MODE - Interactive Text Generation\n",
    "# =============================================================================\n",
    "# This section provides an interactive interface for testing the model's\n",
    "# text generation capabilities. It allows users to input prompts and see\n",
    "# how the model responds, which is useful for evaluating model performance.\n",
    "\n",
    "# =============================================================================\n",
    "# INFERENCE MODE ACTIVATION - Interactive Testing\n",
    "# =============================================================================\n",
    "if inference == True:\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    # Disable dropout and other training-specific behaviors\n",
    "    # This ensures consistent generation behavior\n",
    "    \n",
    "    # =============================================================================\n",
    "    # INTERACTIVE LOOP - Continuous Text Generation\n",
    "    # =============================================================================\n",
    "    while True:\n",
    "        qs = input(\"Enter text (q to quit) >>> \")  # Get user input\n",
    "        \n",
    "        # =============================================================================\n",
    "        # INPUT VALIDATION - Handle Empty Inputs\n",
    "        # =============================================================================\n",
    "        if qs == \"\":  # Skip empty inputs\n",
    "            continue\n",
    "            \n",
    "        # =============================================================================\n",
    "        # EXIT CONDITION - Quit the Interactive Loop\n",
    "        # =============================================================================\n",
    "        if qs == 'q':  # Exit when user types 'q'\n",
    "            break\n",
    "            \n",
    "        # =============================================================================\n",
    "        # TEXT GENERATION - Generate Response to User Input\n",
    "        # =============================================================================\n",
    "        generate_sample(qs)  # Generate text based on user input\n",
    "        # This demonstrates the model's ability to complete or continue text\n",
    "        # The model will try to generate coherent text based on the input prompt\n",
    "        # This is useful for testing the model's understanding and generation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "2zUtal8zwsUl",
    "outputId": "435ac1e0-8bb5-45f6-d17a-8f81e4e444d1"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN TRAINING LOOP - The Heart of Model Learning\n",
    "# =============================================================================\n",
    "# This section contains the main training loop that teaches our model to\n",
    "# understand and generate text. It's the most critical part of the entire\n",
    "# process, where the model learns from data and improves its performance.\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING EXECUTION - Learn from Data\n",
    "# =============================================================================\n",
    "try:\n",
    "    # =============================================================================\n",
    "    # TRAINING ITERATION LOOP - Process Each Batch\n",
    "    # =============================================================================\n",
    "    for i in tqdm(range(start_iteration, train_iters)):\n",
    "        # =============================================================================\n",
    "        # BATCH PREPARATION - Get Training Data\n",
    "        # =============================================================================\n",
    "        xb, yb = get_batch(\"train\")  # Get a new batch of training data\n",
    "        # xb: Input sequences (batch_size, sequence_length)\n",
    "        # yb: Target sequences (batch_size, sequence_length)\n",
    "        \n",
    "        # =============================================================================\n",
    "        # FORWARD PASS - Run the Model\n",
    "        # =============================================================================\n",
    "        logits, loss = model(xb, yb)  # Run the LLM and get the logits and the loss\n",
    "        # logits: Model predictions for each token position\n",
    "        # loss: Cross-entropy loss between predictions and targets\n",
    "        \n",
    "        # =============================================================================\n",
    "        # EVALUATION - Monitor Training Progress\n",
    "        # =============================================================================\n",
    "        if (i % eval_interval == 0 or i == train_iters - 1):  # Calculate the loss\n",
    "            l = calculate_loss()  # Evaluate on both training and validation data\n",
    "            print(f\"\\n{i}: train loss: {l['train']} / val loss: {l['eval']}\")\n",
    "            \n",
    "            # =============================================================================\n",
    "            # TEXT GENERATION TEST - Observe Model Evolution\n",
    "            # =============================================================================\n",
    "            # We do a quick test so that we observe the evolution through the training\n",
    "            # Remember that we use a very small dataset which doesn't include all topics\n",
    "            generate_sample(\"The mountain in my city is\")  # Generate a sample\n",
    "            # This shows how the model's text generation improves over time\n",
    "            # Initially random, gradually becoming more coherent\n",
    "            \n",
    "            # =============================================================================\n",
    "            # CHECKPOINT SAVING - Save Best Model\n",
    "            # =============================================================================\n",
    "            if l['eval'] < best_val_loss:  # If we improved the best loss, save a checkpoint\n",
    "                best_val_loss = l['eval']\n",
    "                print(\"[CHECKPOINT]: Saving with loss: \", best_val_loss)\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),      # Model parameters\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state\n",
    "                    'loss': best_val_loss,                      # Best validation loss\n",
    "                    'iteration': i,                             # Current iteration\n",
    "                }, checkpoint_dir + checkpoint_fn)\n",
    "                # Save the model when validation loss improves\n",
    "                # This ensures we keep the best performing model\n",
    "            \n",
    "            # =============================================================================\n",
    "            # WANDB LOGGING - Track Training Metrics\n",
    "            # =============================================================================\n",
    "            if wandb_log:\n",
    "                wandb.log({\n",
    "                    \"loss/train\": l['train'],      # Training loss\n",
    "                    \"loss/val\": l['eval'],         # Validation loss\n",
    "                    \"lr\": scheduler.get_last_lr()[0],  # Current learning rate\n",
    "                }, step=i)\n",
    "                # Log metrics to Weights & Biases for visualization\n",
    "                # This helps track training progress and identify issues\n",
    "        \n",
    "        # =============================================================================\n",
    "        # BACKWARD PASS - Compute Gradients\n",
    "        # =============================================================================\n",
    "        optimizer.zero_grad(set_to_none=True)  # Reset gradients\n",
    "        # Clear gradients from previous iteration\n",
    "        # set_to_none=True is more memory efficient\n",
    "        \n",
    "        loss.backward()  # Calculate new gradients\n",
    "        # Compute gradients of loss with respect to all parameters\n",
    "        # This tells us how to adjust each parameter to reduce the loss\n",
    "        \n",
    "        # =============================================================================\n",
    "        # GRADIENT CLIPPING - Prevent Exploding Gradients\n",
    "        # =============================================================================\n",
    "        # This line clips the gradients to prevent the exploding gradient problem during training.\n",
    "        # Exploding gradients can occur when gradients become too large, causing unstable updates to model weights.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "        # Clip gradients to maximum norm of grad_clip\n",
    "        # This prevents gradients from becoming too large and destabilizing training\n",
    "        \n",
    "        # =============================================================================\n",
    "        # PARAMETER UPDATE - Apply Gradients\n",
    "        # =============================================================================\n",
    "        optimizer.step()  # Update the model parameters\n",
    "        # Apply the computed gradients to update model weights\n",
    "        # This is where the actual learning happens\n",
    "        \n",
    "        scheduler.step()  # Update the learning rate value\n",
    "        # Adjust learning rate according to the schedule\n",
    "        # This helps with convergence and can improve final performance\n",
    "    \n",
    "    # =============================================================================\n",
    "    # TRAINING COMPLETION - Finish Logging\n",
    "    # =============================================================================\n",
    "    if wandb_log:\n",
    "        wandb.finish()  # Close wandb session\n",
    "        # Finalize experiment tracking\n",
    "\n",
    "# =============================================================================\n",
    "# EXCEPTION HANDLING - Graceful Training Interruption\n",
    "# =============================================================================\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Cleaning up...\")\n",
    "    # Handle Ctrl+C interruption gracefully\n",
    "    # This allows users to stop training without losing progress\n",
    "\n",
    "# =============================================================================\n",
    "# CLEANUP - Release Resources\n",
    "# =============================================================================\n",
    "finally:\n",
    "    # Release GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory released.\")\n",
    "    # Clear GPU memory to free up resources\n",
    "    # This is important for preventing memory leaks\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL CLEANUP - Ensure All Resources are Released\n",
    "# =============================================================================\n",
    "if wandb_log:   \n",
    "    wandb.finish()  # Ensure wandb is properly closed\n",
    "torch.cuda.empty_cache()  # Final GPU memory cleanup\n",
    "\n",
    "# =============================================================================\n",
    "# CREDITS - Acknowledgment\n",
    "# =============================================================================\n",
    "# Code designed by Javier ideami\n",
    "# ideami.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnYRMN1-xAtK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
