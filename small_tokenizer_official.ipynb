{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071695ad-75e5-4163-a8e8-5a9163af6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run the following installation lines ONLY if you havent installed these libraries already outside of the notebook\n",
    "#!pip install ipdb -q\n",
    "#!pip install sentencepiece -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a31016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file will train a tokenizer on the small wiki.txt dataset\n",
    "# and save the tokenizer in the files {model_prefix}.model and {model_prefix}.vocab\n",
    "\n",
    "# Import libraries\n",
    "import sentencepiece as spm\n",
    "import os, sys\n",
    "\n",
    "vocab_size = 4096 # Size of the vocabulary you wish to have\n",
    "\n",
    "# Official notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55921221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this online (for example at Google Colab), \n",
    "# make sure you have the support files on the same folder\n",
    "# Otherwise run this cell to download them\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created.\n",
    "\n",
    "import os, requests, zipfile, io \n",
    "\n",
    "files_url = \"https://ideami.com/llm_train\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"encoded_data.pt\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the encoded_data.pt file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da55d9dd-15c1-4b04-9f68-b2159cc8f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed\n"
     ]
    }
   ],
   "source": [
    "# Training the Sentence Piece Tokenizer\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "        input='wiki.txt',\n",
    "        model_prefix=\"test_wiki_tokenizer\", # pick the name for your trained tokenizer\n",
    "        model_type=\"bpe\",\n",
    "        vocab_size=vocab_size,\n",
    "        self_test_sample_size=0,\n",
    "        input_format=\"text\",\n",
    "        character_coverage=0.995,\n",
    "        num_threads=os.cpu_count(),\n",
    "        split_digits=True,\n",
    "        allow_whitespace_only_pieces=True,\n",
    "        byte_fallback=True,\n",
    "        unk_surface=r\" \\342\\201\\207 \",\n",
    "        normalization_rule_name=\"identity\"\n",
    "    )\n",
    "\n",
    "print(\"Tokenizer training completed\")\n",
    "\n",
    "# The character_coverage parameter specifies the proportion of characters in the training corpus\n",
    "# that are considered when building the tokenizer model. This is important for languages with\n",
    "# large character sets, such as Japanese or Chinese, where it is impractical to include all characters.\n",
    "# A character_coverage value of 0.995 means that the tokenizer will include the most frequent 99.5%\n",
    "# of characters in the training corpus. The remaining 0.5% of less frequent characters will be\n",
    "# treated as unknown. This helps in managing the vocabulary size and ensures that the tokenizer\n",
    "# focuses on the most common characters, improving efficiency and performance.\n",
    "\n",
    "# The model_type parameter determines the algorithm used to create the tokenizer model.\n",
    "# You can choose from several types, including bpe (Byte Pair Encoding), unigram, word, and char.\n",
    "# BPE stands for Byte Pair Encoding. BPE is a subword tokenization algorithm that iteratively merges\n",
    "# the most frequent pairs of bytes (or characters) in the corpus to form subwords. This process continues\n",
    "# until the desired vocabulary size is reached. BPE can split rare words into more frequent subword units,\n",
    "# improving the model's ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0bf19b-05b2-4874-9193-5993a76fb5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece vocab_size: 4096\n",
      "[1233, 275, 299, 261, 2682, 4049, 297, 460, 392, 2126, 353, 2347, 382, 511, 66]\n",
      "What is a healthy dish that includes strawberry?\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## Validate that training was successful\n",
    "\n",
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor(model_file='test_wiki_tokenizer.model')\n",
    "\n",
    "# Print Vocabulary Size\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"SentencePiece vocab_size: {vocab_size}\")\n",
    "\n",
    "# Create helper encoding/decoding Functions\n",
    "encode = lambda s: sp.Encode(s)  \n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "# Test the encoding and decoding functions\n",
    "print(encode(\"What is a healthy dish that includes strawberry?\"))\n",
    "print(decode(encode(\"What is a healthy dish that includes strawberry?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d896f-dc71-4652-a2d9-3b074d3416aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
