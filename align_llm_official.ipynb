{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd51f6-e9df-4ae0-91a8-9fe80040ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Alignment with ORPO Technique - Notebook created by Javier Ideami (ideami.com)\n",
    "# ORPO: Monolithic Preference Optimization without Reference Model (https://arxiv.org/abs/2403.07691)\n",
    "\n",
    "## What is this notebook about?\n",
    "# This notebook demonstrates how to align a Large Language Model (LLM) using the ORPO (Odds Ratio Preference Optimization) technique.\n",
    "# ORPO is a modern approach to AI alignment that teaches models to prefer helpful, harmless, and honest responses over harmful ones.\n",
    "\n",
    "## Why do we need LLM alignment?\n",
    "# LLM Alignment needs to be applied to an LLM that is a bit more powerful than the basic one we trained earlier.\n",
    "# That's why I am providing the checkpoint of an already trained more powerful LLM (138 million parameters vs 19 million of ours)\n",
    "# so that we can apply ORPO alignment on top of it.\n",
    "# The pretrained 138 million parameter model has been pre-trained on the open source Fineweb-Edu dataset.\n",
    "\n",
    "## What makes ORPO special?\n",
    "# Traditional alignment techniques like RLHF (Reinforcement Learning from Human Feedback) take a long time and money to be implemented. \n",
    "# New techniques like ORPO simplify alignment a lot.\n",
    "# We will be able to train ORPO alignment with 1 single GPU and little GPU memory (around 4 gigabytes)  \n",
    "\n",
    "## What to expect from this tutorial?\n",
    "# Of course results will be imperfect because we are still applying ORPO on top of a model that is small and has been trained with much\n",
    "# less data than the large models out there, and for a much shorter amount of time. But it will be enough to compare the before and after,\n",
    "# and to see how ORPO improves the way the LLM communicates in many occasions.\n",
    "# And we will understand every part of the code that makes it possible.\n",
    "\n",
    "## License Information\n",
    "# This file includes code from the open source ORPO repository licensed under the Apache License 2.0.\n",
    "# See licenses/orpo-license for details.\n",
    "# Modifications: Variable names have been changed for educational purposes.\n",
    "\n",
    "# This file also uses the ORPO-DPO-mix-40k dataset licensed under the Apache License 2.0.\n",
    "# See licenses/orpo-dpo-mix-40k-license for details.\n",
    "\n",
    "# Official notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üö® IMPORTANT: GPU Requirements\n",
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically\n",
    "\n",
    "## Why do we need a GPU?\n",
    "# - ORPO training involves complex mathematical operations on large neural networks\n",
    "# - CPU processing would take hours or days to complete\n",
    "# - GPU parallel processing reduces training time from hours to minutes\n",
    "# - We need at least 4GB of GPU memory for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678006f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ INSTALLATION SECTION\n",
    "# Uncomment and run the following installation lines ONLY if you haven't installed these libraries already outside of the notebook\n",
    "\n",
    "# Core libraries for debugging and progress tracking\n",
    "#!pip install -q ipdb        # Interactive Python debugger for troubleshooting\n",
    "#!pip install -q tqdm        # Progress bars for training loops\n",
    "\n",
    "# Hugging Face ecosystem for datasets and models\n",
    "#!pip install -q datasets    # Library for loading and processing datasets\n",
    "#!pip install -q transformers # Hugging Face transformers library for model handling\n",
    "\n",
    "# Experiment tracking and visualization\n",
    "#!pip install -q wandb       # Weights & Biases for experiment logging and visualization\n",
    "\n",
    "# PyTorch installation (if not already installed)\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# Find the ideal pytorch installation command at https://pytorch.org/get-started/locally/\n",
    "\n",
    "# Optional performance optimization\n",
    "# In addition, you may try to install the flash-attn library, although at the moment it is having issues\n",
    "# Flash Attention can speed up training but may have compatibility issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1308c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç GPU MEMORY CHECK\n",
    "# This command shows your GPU information and available memory\n",
    "# Make sure that you have at least 4GB of free GPU memory to run this tutorial successfully\n",
    "\n",
    "!nvidia-smi \n",
    "\n",
    "# üìã What to look for in the output:\n",
    "# - GPU Name: Should show your GPU model (e.g., Tesla T4, V100, A100)\n",
    "# - Memory Usage: Should show available memory (we need at least 4GB free)\n",
    "# - Driver Version: Should be compatible with PyTorch\n",
    "\n",
    "# If you are using Google Colab or a similar online platform, make sure to select a GPU in the menus\n",
    "# In Google Colab, the option is within the Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ab151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary files and create necessary folders\n",
    "# llm.py - llm model: an llm architecture that is more powerful\n",
    "# models folder: pretrained checkpoints for the base and the aligned model\n",
    "# data folder: tokenized dataset stored in this folder\n",
    "# tokenizers folder: pretrained tokenizer on the large dataset FineWeb-Edu\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created. If you have any problems downloading the files with this code, I have also added llm_align.zip\n",
    "# to the downloadable resources of this lecture (however, best option is to use this code, because then you don't need\n",
    "# to upload the files or do anything else)\n",
    "\n",
    "import os, requests, zipfile, io \n",
    "\n",
    "files_url = \"https://ideami.com/llm_align\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"llm.py\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db2545-dc98-4d71-b259-78ee59dfa753",
   "metadata": {},
   "outputs": [],
   "source": [
    "### üìö IMPORT NECESSARY LIBRARIES\n",
    "\n",
    "# Standard Python libraries for file operations and system utilities\n",
    "import os          # File and directory operations\n",
    "import sys         # System-specific parameters and functions\n",
    "import math        # Mathematical functions (needed for learning rate scheduling)\n",
    "from tqdm import tqdm        # Progress bars for training loops\n",
    "from datetime import datetime # Timestamp generation for experiment tracking\n",
    "import ipdb        # Interactive debugger for troubleshooting\n",
    "\n",
    "# Type hints for better code documentation and IDE support\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "# üß† PYTORCH - Core deep learning framework\n",
    "import torch           # Main PyTorch library\n",
    "import torch.nn as nn  # Neural network modules and layers\n",
    "from torch.nn import functional as F  # Functional interface for neural network operations\n",
    "\n",
    "# ü§ó HUGGING FACE - Pre-trained models and datasets\n",
    "import transformers    # Hugging Face transformers library for model handling\n",
    "from datasets import load_dataset, load_from_disk  # Dataset loading utilities\n",
    "\n",
    "# ‚ö° PERFORMANCE OPTIMIZATIONS\n",
    "# These lines improve performance for Ampere Architecture GPUs (e.g: A100s, RTX 30/40 series)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Allow tf32 precision for matrix multiplications (faster)\n",
    "torch.backends.cudnn.allow_tf32 = True        # Allow tf32 precision for cuDNN operations (faster)\n",
    "# Empty GPU cache memory to start with a clean slate\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# üîß DEBUGGING SETTINGS\n",
    "# Optional settings for debugging - makes tensor printing more readable\n",
    "torch.set_printoptions(threshold=10000)      # Show more tensor elements when printing\n",
    "torch.set_printoptions(sci_mode=False, precision=2)  # Use decimal notation instead of scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054f1b2-d06f-4165-8d71-75feac4b2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéõÔ∏è TRAINING PARAMETERS - Core settings that control the training process\n",
    "batch_size = 1  # Number of examples processed together (you can change it to 2 if you have enough GPU memory)\n",
    "epochs = 3      # Number of complete passes through the training dataset\n",
    "lr = 6e-5       # Learning rate - how fast the model learns (6e-5 = 0.00006)\n",
    "lr_warmup_steps = 100  # Number of steps to gradually increase learning rate from 0 to full value\n",
    "context = 1024  # Maximum sequence length the model can handle (in tokens)\n",
    "alpha = 0.5     # Weighting factor for the ORPO odds ratio (balances standard loss vs preference loss)\n",
    "prompt_max_size = 512  # Maximum length for the prompt part (leaves room for the response)\n",
    "compile = False # PyTorch compilation for performance (disable if your system doesn't support it)\n",
    "dtype = torch.bfloat16  # Data type for calculations (bfloat16 is faster and uses less memory than float32)\n",
    "log_iters = 100 # How often to print training progress and metrics\n",
    "\n",
    "# üîß HYPERPARAMETERS - Advanced training settings\n",
    "dropout = 0.    # Dropout rate (0 = no dropout, 1 = maximum dropout)\n",
    "grad_clip = 1.0 # Maximum gradient norm (prevents exploding gradients)\n",
    "weight_decay = 0.0  # L2 regularization strength (0 = no regularization)\n",
    "\n",
    "# üíª DEVICE CONFIGURATION - Choose between GPU and CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: You will be using: \",device)\n",
    "\n",
    "# üìä What these parameters mean:\n",
    "# - batch_size: Larger batches = more stable training but need more GPU memory\n",
    "# - epochs: More epochs = better learning but risk of overfitting\n",
    "# - lr: Higher learning rate = faster learning but risk of instability\n",
    "# - context: Longer sequences = more context but need more memory\n",
    "# - alpha: Higher alpha = more focus on preference learning vs standard language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810053b-5e06-49ef-8a7c-2b4988f6b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä EXPERIMENT LOGGING - Track training progress and metrics\n",
    "project_name=\"test\"  # Name for organizing your experiments\n",
    "wandb_log = True    # Enable Weights & Biases logging (set to False to disable)\n",
    "wandb_project = project_name  # Project name in W&B dashboard\n",
    "wandb_run_name = \"test-run\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")  # Unique run name with timestamp\n",
    "\n",
    "# Initialize Weights & Biases for experiment tracking\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "\n",
    "# üîë W&B SETUP INSTRUCTIONS:\n",
    "# The first time you run this logging code with wandb_log=True, the weights and biases library\n",
    "# will ask you for an API key. You can follow the instructions in the video, or you can\n",
    "# also simply click on a link that should appear when you run this cell, pointing to this\n",
    "# address: https://wandb.ai/authorize  \n",
    "# Going to that address will allow you to quickly get an API key as well\n",
    "\n",
    "# üìà What W&B tracks:\n",
    "# - Training and validation losses over time\n",
    "# - Learning rate schedules\n",
    "# - Model performance metrics\n",
    "# - GPU memory usage\n",
    "# - Training speed and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77f66f-e3c7-4ea0-ba33-fc8792827ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ DATASET PATHS - Where to find and store our training data\n",
    "dataset_path = \"./data/orpo_dataset\"  # Local directory where the processed ORPO dataset will be stored\n",
    "\n",
    "# üåê HUGGING FACE DATASET\n",
    "# This is a special dataset prepared for ORPO alignment training. It contains:\n",
    "# - Paired examples of \"chosen\" (good) vs \"rejected\" (bad) responses\n",
    "# - Human preferences for AI assistant responses\n",
    "# - Available at: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k/blob/main/README.md\n",
    "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"  # HuggingFace dataset identifier\n",
    "\n",
    "# üî§ TOKENIZER AND MODEL PATHS\n",
    "tokenizer_path = \"tokenizers/tok16384\"  # Path to the pre-trained tokenizer (converts text to numbers)\n",
    "checkpoint_dir = './models/'           # Directory where model checkpoints are stored\n",
    "\n",
    "# üìã What we'll download:\n",
    "# - llm.py: The neural network architecture code\n",
    "# - models/: Pre-trained model weights (base and aligned versions)\n",
    "# - data/: Processed training dataset\n",
    "# - tokenizers/: Text tokenizer for converting words to numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a8b95-0b03-4e26-ae35-0e9d6a82f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî§ TOKENIZER SETUP - Convert text to numbers that the model can understand\n",
    "###########\n",
    "# Load the pre-trained tokenizer (converts text to token IDs and vice versa)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# üí¨ CHAT TEMPLATE CONFIGURATION\n",
    "# Set up a template that formats conversations between user and assistant\n",
    "# This template adds special tokens like <|user|>, <|assistant|> to mark different speakers\n",
    "tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "# üîß TOKENIZER CONFIGURATION\n",
    "# Make padding token equal to the end of sentence token (which has id of 2 in this case)\n",
    "# This ensures consistent padding behavior during training\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# üîÑ DATASET LOADING STRATEGY\n",
    "# If you want to debug the tokenization of the dataset, delete the orpo_dataset folder or change\n",
    "# this dataset_path to something else like \"./data/tmp\"\n",
    "# uncomment to activate the preprocessing of dataset\n",
    "#dataset_path = \"./data/tmp\" # delete this folder to preprocess again\n",
    "\n",
    "# üìÇ SMART DATASET LOADING\n",
    "# If dataset has already been processed and tokenized, we can load it directly from our disk\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"‚úÖ Loading pre-processed dataset from disk (faster!)\")\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "# Otherwise, we will load the dataset from HuggingFace and then filter and tokenize it\n",
    "else:\n",
    "    print(\"üîÑ Preprocessing and tokenizing dataset (this may take a while...)\")\n",
    "    dataset = load_dataset(dataset_name, split=\"all\")\n",
    "\n",
    "    # üö´ CONTENT FILTERING\n",
    "    # Optional: Filter out toxic entries to improve training quality\n",
    "    # Without this filter: ~37,136 elements, with filter: ~36,622 elements\n",
    "    dataset = dataset.filter(lambda r: r[\"source\"] != \"toxic-dpo-v0.2\")\n",
    "\n",
    "    # üîç DATASET FILTERING FUNCTION\n",
    "    # This function eliminates entries that are too long to fit in our context window\n",
    "    # We need prompt + answer to fit within the total context (1024 tokens)\n",
    "    def filter_dataset(examples):\n",
    "        # examples['chosen'][:-1] picks the prompt minus the final answer\n",
    "        # This gives us just the conversation history without the response we want to predict\n",
    "        prompt_length = tokenizer.apply_chat_template(examples['chosen'][:-1], tokenize=True, add_generation_prompt=True, return_tensors='pt').size(-1)  \n",
    "        \n",
    "        # Preserve only samples that have a prompt smaller than prompt_max_size (512 tokens)\n",
    "        # This leaves room for the response within our 1024 token limit\n",
    "        if prompt_length < prompt_max_size:    \n",
    "            return True  # Keep this example\n",
    "        else:\n",
    "            return False  # Skip this example (too long)\n",
    "\n",
    "\n",
    "    # üîÑ MAIN DATASET PREPROCESSING FUNCTION\n",
    "    # This function converts raw text conversations into tokenized format for training\n",
    "    # HF Tokenizer Dict Format: Encoding(num_tokens=1024, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "    def preprocess_dataset(examples: Union[List, Dict]):\n",
    "        # Process examples in batches of 1000 by default for efficiency\n",
    "        \n",
    "        # üìù PROMPT EXTRACTION\n",
    "        # Take chosen field, eliminate last answer, apply template adding assistant prompt\n",
    "        # This creates the \"prompt\" - everything the user said plus conversation history\n",
    "        prompt = [tokenizer.apply_chat_template(item[:-1], tokenize=False, add_generation_prompt=True) for item in examples['chosen']]\n",
    "        \n",
    "        # üí° IMPORTANT: Multi-turn conversations\n",
    "        # Some samples are multi-turn conversations. The prompt includes all interactions between user and assistant\n",
    "        # until the last question. We remove the last answer and all previous interactions become the prompt.\n",
    "\n",
    "        # ‚úÖ CHOSEN RESPONSE (Good answer)\n",
    "        # Take the chosen field (the preferred response), then apply chat template \n",
    "        chosen = [tokenizer.apply_chat_template(item, tokenize=False) for item in examples['chosen']]\n",
    "\n",
    "        # ‚ùå REJECTED RESPONSE (Bad answer)\n",
    "        # Take the rejected field (the dispreferred response), then apply chat template \n",
    "        rejected = [tokenizer.apply_chat_template(item, tokenize=False) for item in examples['rejected']]\n",
    "        \n",
    "        # üî¢ TOKENIZATION - Convert text to numbers\n",
    "        # Tokenize the prompt (conversation history)\n",
    "        inputs = tokenizer(prompt, max_length=context, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        # inputs will have dict fields: input_ids and attention_mask (e.g: inputs.input_ids[0])\n",
    "        # You can test by decoding: tokenizer.decode(inputs.input_ids[0])\n",
    "\n",
    "        # üìè PADDING EXPLANATION\n",
    "        # All elements will have the same length of 1024 tokens\n",
    "        # Extra padding tokens will be added to reach 1024 for shorter sequences\n",
    "\n",
    "        # ‚úÖ TOKENIZE CHOSEN RESPONSE (Good answer)\n",
    "        pos_labels = tokenizer(chosen, max_length=context, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        # pos_labels will have dict fields: input_ids and attention_mask (e.g: pos_labels.input_ids[0])\n",
    "        # You can test by decoding: tokenizer.decode(pos_labels.input_ids[0])\n",
    "        \n",
    "        # ‚ùå TOKENIZE REJECTED RESPONSE (Bad answer)\n",
    "        neg_labels = tokenizer(rejected, max_length=context, padding='max_length', truncation=True, return_tensors='pt') \n",
    "        # Same structure as pos_labels above\n",
    "\n",
    "        # üîó COMBINE ALL DATA INTO SINGLE DICTIONARY\n",
    "        # Add the chosen-positive and rejected-negative response ids and masks to the prompt data\n",
    "        # This creates a unified structure containing all the information we need for ORPO training\n",
    "        inputs['positive_input_ids'] = pos_labels['input_ids']        # Token IDs for good responses\n",
    "        inputs['positive_attention_mask'] = pos_labels['attention_mask']  # Attention mask for good responses\n",
    "\n",
    "        inputs['negative_input_ids'] = neg_labels['input_ids']         # Token IDs for bad responses  \n",
    "        inputs['negative_attention_mask'] = neg_labels['attention_mask']  # Attention mask for bad responses\n",
    "\n",
    "        return inputs  # Return the complete dataset entry\n",
    "    \n",
    "    # üîç APPLY FILTERING\n",
    "    # Filter dataset to exclude prompts that are too long for our context window\n",
    "    dataset = dataset.filter(filter_dataset)\n",
    "\n",
    "    # üîÑ APPLY PREPROCESSING\n",
    "    # Preprocess the dataset (tokenize and format all examples)\n",
    "    # If you have issues with multiprocessing, make sure to use num_proc=1\n",
    "    # Multiprocessing alternative: dataset = dataset.map(preprocess_dataset, batched=True, num_proc=min(32,os.cpu_count()), remove_columns=dataset.column_names)  \n",
    "    dataset = dataset.map(preprocess_dataset, batched=True, num_proc=1, remove_columns=dataset.column_names) \n",
    "    # Processed in batches of 1000 by default for efficiency\n",
    "\n",
    "    # üìä FINAL DATASET STRUCTURE\n",
    "    # As a result of the preprocessing, dataset variable will have all these internal fields:\n",
    "    # Dataset({features: ['input_ids', 'token_type_ids', 'attention_mask', 'positive_input_ids', 'positive_attention_mask', 'negative_input_ids', 'negative_attention_mask'], num_rows: 39091})\n",
    "\n",
    "    # üíæ SAVE PROCESSED DATASET\n",
    "    # Save the processed dataset to disk for faster loading in future runs\n",
    "    dataset.save_to_disk(dataset_path)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d7d0b-33a3-45b5-a728-59c19c815c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, you can test some of the content of the dataset with for example these: \n",
    "# dataset[0]['input_ids']  /  dataset[0]['positive_input_ids']\n",
    "# testing Ids to Text: tokenizer.decode(dataset[0]['positive_input_ids'])\n",
    "\n",
    "# Split the data into train and validation, 5% for the validation set\n",
    "dataset = dataset.shuffle(42).train_test_split(test_size=0.05)  \n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "#Dataset({features: ['input_ids', 'token_type_ids', 'attention_mask', 'positive_input_ids', 'positive_attention_mask', 'negative_input_ids', 'negative_attention_mask'],num_rows: 37136})\n",
    "\n",
    "val_data = dataset[\"test\"]\n",
    "#Dataset({features: ['input_ids', 'token_type_ids', 'attention_mask', 'positive_input_ids', 'positive_attention_mask', 'negative_input_ids', 'negative_attention_mask'],num_rows: 1955})\n",
    "\n",
    "# Data_collator efficiently prepares your training and validation data for language modeling by batching, padding (optional), and performing masking (optional) according to your configuration\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Setup DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, collate_fn=data_collator, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=data_collator, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d7400-41b2-474a-b31b-50538f038b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Test your DataLoaders\n",
    "#batch = next(iter(train_loader))\n",
    "#print(tokenizer.decode(batch['input_ids'][0]))  # this will show just the prompt, followed by padding tokens\n",
    "\n",
    "# debug: batch['input_ids'] (shape is 1,1024)  \n",
    "# debug: Ids to Text -> tokenizer.decode(batch['input_ids'][0]) - [0] because needs to address inside batch number dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f9a838-ab30-49f3-9d70-135f55e2bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "###############################################################\n",
    "################### üß† SETUP MODEL ############################\n",
    "###############################################################\n",
    "###############################################################\n",
    "\n",
    "# ü¶ô IMPORT LLAMA-BASED MODEL ARCHITECTURE\n",
    "# Import the Llama model class and configuration from our custom implementation\n",
    "from llm import Llama, ModelArgs\n",
    "\n",
    "# üìÇ LOAD PRETRAINED MODEL CHECKPOINT\n",
    "# Load the pre-trained model weights (138 million parameters)\n",
    "# This model was trained on the FineWeb-Edu dataset and is ready for alignment\n",
    "checkpoint = torch.load(os.path.join(checkpoint_dir, \"base_model.pt\"))\n",
    "\n",
    "# ‚öôÔ∏è EXTRACT MODEL CONFIGURATION\n",
    "# Remove the config from checkpoint and store it separately\n",
    "# The config contains all the architectural parameters (layers, heads, dimensions, etc.)\n",
    "config = checkpoint.pop(\"config\")\n",
    "\n",
    "# üèóÔ∏è CREATE MODEL CONFIGURATION\n",
    "# Instantiate ModelArgs with the necessary parameters from the loaded config\n",
    "model_args = ModelArgs(\n",
    "    dim=config.hidden_size,                    # Hidden dimension (768)\n",
    "    n_layers=config.num_hidden_layers,        # Number of transformer layers (12)\n",
    "    n_heads=config.num_attention_heads,        # Number of attention heads (12)\n",
    "    n_kv_heads=config.num_key_value_heads,    # Number of key-value heads (12)\n",
    "    vocab_size=config.vocab_size,              # Vocabulary size (16384)\n",
    "    norm_eps=config.rms_norm_eps,             # RMS normalization epsilon (1e-06)\n",
    "    rope_theta=config.rope_theta,             # RoPE theta parameter (10000.0)\n",
    "    max_seq_len=context,                       # Maximum sequence length (1024)\n",
    "    dropout=config.attention_dropout,          # Dropout rate (0.0)\n",
    "    hidden_dim=config.intermediate_size,       # Feed-forward hidden dimension (3072)\n",
    "    attention_bias=config.attention_bias,      # Whether to use attention bias (False)\n",
    "    mlp_bias=config.mlp_bias                   # Whether to use MLP bias (False)\n",
    ")\n",
    "\n",
    "# üìä MODEL ARCHITECTURE SUMMARY\n",
    "# ModelArgs(dim=768, n_layers=12, n_heads=12, n_kv_heads=12, vocab_size=16384, \n",
    "#          norm_eps=1e-06, rope_theta=10000.0, max_seq_len=1024, dropout=0.0, \n",
    "#          hidden_dim=3072, attention_bias=False, mlp_bias=False)\n",
    "\n",
    "# üöÄ INSTANTIATE AND LOAD MODEL\n",
    "model = Llama(model_args)           # Create the model with our configuration\n",
    "model.load_state_dict(checkpoint)   # Load the pre-trained weights into the model\n",
    "\n",
    "# ‚ö° MODEL OPTIMIZATION AND DEVICE SETUP\n",
    "model = model.to(dtype)   # Set the precision type (bfloat16 for faster training)\n",
    "model = model.to(device)  # Move the model to GPU (or CPU if GPU not available)\n",
    "\n",
    "# üéØ SET TRAINING MODE\n",
    "model.train()  # Enable training mode (enables dropout, batch norm updates, etc.)\n",
    "\n",
    "# üöÄ OPTIONAL: MODEL COMPILATION\n",
    "# Torch.compile compiles a PyTorch model to an optimized version for better performance\n",
    "# This can significantly speed up training but may not be supported on all systems\n",
    "if compile:\n",
    "    print(\"[INFO] Compiling model for optimal performance\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# üìä MODEL SIZE INFORMATION\n",
    "# Print the total number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model has {total_params / 1e6:.1f} Million parameters\")\n",
    "print(f\"Model architecture: {config.num_hidden_layers} layers, {config.hidden_size} hidden size\")\n",
    "print(f\"Model is ready for ORPO alignment training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99e1e1-38b5-4e95-a8e4-bfb1ac4e49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "########## üéØ SETUP TRAINING AND OPTIMIZER ############\n",
    "#######################################################\n",
    "\n",
    "# üöÄ OPTIMIZER CONFIGURATION\n",
    "# Declare optimizer - helps us compute gradients, update parameters, manage learning rate, apply weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=lr,                    # Learning rate (6e-5)\n",
    "    betas=(0.9, 0.95),        # Control exponential moving averages of gradient and its square (AdamW algorithm)\n",
    "    eps=1e-8,                 # Small number for numerical stability in computations\n",
    "    fused=device == 'cuda',   # Fused operations for better GPU performance (combines multiple operations)\n",
    "    weight_decay=weight_decay # L2 regularization strength\n",
    ")\n",
    "\n",
    "# üìä TRAINING SCHEDULE CALCULATION\n",
    "# Calculate total number of training steps: length of training loader √ó number of epochs\n",
    "num_training_steps = len(train_loader) * epochs  # 111,408 with default settings (batch size = 1)\n",
    "\n",
    "# üìà LEARNING RATE SCHEDULER\n",
    "# First 100 steps: linear warmup (gradually increase LR from 0 to full value)\n",
    "# After warmup: cosine decay (gradually decrease LR following a cosine curve)\n",
    "def lr_lambda(current_step):\n",
    "    # Linear warmup phase\n",
    "    if current_step < lr_warmup_steps:\n",
    "        return float(current_step) / float(max(1, lr_warmup_steps))\n",
    "    \n",
    "    # Cosine decay phase\n",
    "    progress = float(current_step - lr_warmup_steps) / float(max(1, num_training_steps - lr_warmup_steps))\n",
    "    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(0.5) * 2.0 * progress)))\n",
    "\n",
    "# üîß CREATE LEARNING RATE SCHEDULER\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bbdb8-4f91-4aca-b5f8-7281243c1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßÆ COMPUTE LOG PROBABILITIES - Core function for ORPO odds ratio calculation\n",
    "# This function calculates the log probabilities for chosen responses, necessary for Log Odds Calculation\n",
    "def compute_logps(prompt_attention_mask, chosen_inputs, chosen_attention_mask, logits):\n",
    "    \"\"\"\n",
    "    Calculate the average log probability of the chosen response tokens.\n",
    "    \n",
    "    Args:\n",
    "        prompt_attention_mask: Mask for prompt tokens (1s where prompt exists, 0s elsewhere)\n",
    "        chosen_inputs: Token IDs of the chosen response\n",
    "        chosen_attention_mask: Mask for chosen response tokens\n",
    "        logits: Model predictions (logits) for all tokens\n",
    "    \n",
    "    Returns:\n",
    "        Average log probability of the chosen response\n",
    "    \"\"\"\n",
    "\n",
    "    # üìù LABEL SHIFTING EXPLANATION\n",
    "    # In general, we get rid of the first element in labels because we want to match each token \n",
    "    # to the label in the next position (we shift labels one to the left).\n",
    "    # As a consequence, we get rid of the last element of logits to equalize dimensions\n",
    "    # and also because we don't care about predictions for the last token (no next token after that)\n",
    "\n",
    "    # üéØ CREATE RESPONSE MASK\n",
    "    # Create mask with only positions of the last answer, starting from the character before the last answer\n",
    "    # because we will start predicting from that position\n",
    "    mask = chosen_attention_mask[:, :-1] - prompt_attention_mask[:, 1:]\n",
    "\n",
    "    # üîç GATHER LOG PROBABILITIES\n",
    "    # torch.gather selects elements of logits based on the indices in index_tensor along dimension 2\n",
    "    # For example: if index gives us token 1160, we go to logits and extract the probability of token 1160\n",
    "    # IMPORTANT: log_softmax function already incorporates the negative sign, so it produces negative log probabilities\n",
    "    # logits[:,:-1,:] shape: (1, 1023, 16384)\n",
    "    # index = (mask * chosen_inputs[:, 1:]).unsqueeze(2) shape: (1, 1023, 1)\n",
    "    # final result: per_token_logps shape: (1, 1023)\n",
    "    per_token_logps = torch.gather(logits[:, :-1, :].log_softmax(-1), dim=2, \n",
    "                                    index=(mask * chosen_inputs[:, 1:]).unsqueeze(2)).squeeze(2)\n",
    "\n",
    "    # üìä NORMALIZE BY RESPONSE LENGTH\n",
    "    # Mask the per_token_logps to leave only positions of the last answer, then normalize\n",
    "    # mask.sum will only sum the active elements so that we normalize by the total tokens of the answer\n",
    "    return torch.mul(per_token_logps, mask.to(dtype)).sum(dim=1).to(dtype) / mask.sum(dim=1).to(dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df56da9-4ef8-42c4-aa0e-0ac4f35c9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def compute_logps(self, prompt_attention_mask, chosen_inputs, chosen_attention_mask, logits):\n",
    "        #mask = chosen_attention_mask[:, :-1] - prompt_attention_mask[:, 1:]\n",
    "        #per_token_logps = torch.gather(logits[:, :-1, :].log_softmax(-1), dim=2, \n",
    "        #                               index=(mask * chosen_inputs[:, 1:]).unsqueeze(2)).squeeze(2)\n",
    "        #return torch.mul(per_token_logps, mask.to(dtype=torch.bfloat16)).sum(dim=1).to(dtype=torch.float64) / mask.sum(dim=1).to(dtype=torch.float64)\n",
    "\n",
    "# this is the code of the toy example we create in the videos in order\n",
    "# to understand in depth how the compute_logps function works\n",
    "# if you wish to run it, uncomment it (you need to uncomment the three quotes at the beginning and also at the end of the code)\n",
    "\n",
    "'''\n",
    "p_mask = torch.tensor([1,1,1,1,0,0,0,0,0,0,0])\n",
    "c_inputs = torch.tensor([2,4,3,2,4,2,1,0,0,0,0])\n",
    "c_mask = torch.tensor([1,1,1,1,1,1,1,0,0,0,0])\n",
    "print(f\"p_mask: {p_mask}\")\n",
    "print(f\"c_inputs: {c_inputs}\")\n",
    "print(f\"c_mask: {c_mask}\")\n",
    "mask = c_mask[:-1] - p_mask[1:] \n",
    "print(f\"mask: {mask}\")\n",
    "logits = torch.tensor([\n",
    "    [0.2, 0.4, 0.8, 0.1, 0.3],\n",
    "    [0.2, 0.1, 0.5, 0.12, 0.31],\n",
    "    [0.22, 0.44, 0.81, 0.13, 0.32],\n",
    "    [0.29, 0.42, 0.84, 0.15, 0.32],\n",
    "    [0.24, 0.48, 0.88, 0.17, 0.34],\n",
    "    [0.21, 0.41, 0.81, 0.14, 0.33],\n",
    "    [0.23, 0.43, 0.82, 0.16, 0.35],\n",
    "    [0.2, 0.4, 0.8, 0.1, 0.3],\n",
    "    [0.2, 0.1, 0.5, 0.12, 0.31],\n",
    "    [0.22, 0.44, 0.81, 0.13, 0.32],\n",
    "    [0.22, 0.44, 0.81, 0.13, 0.32]\n",
    "])\n",
    "\n",
    "print(f\"c_inputs[1:]: {c_inputs[1:]}\")\n",
    "index=(mask * c_inputs[1:])\n",
    "print(f\"index: {index}\")\n",
    "\n",
    "# Expand dimensions for correct gather shape\n",
    "index_expanded = index.unsqueeze(1)\n",
    "print(f\"index_expanded: {index_expanded}\")\n",
    "\n",
    "print(\"shapes: \",index_expanded.shape, logits.shape)\n",
    "# Gather the values at the specified indices\n",
    "gathered_values = torch.gather(logits[:-1,:], dim=1, index=index_expanded)\n",
    "print(f\"gathered: {gathered_values}\")\n",
    "\n",
    "# Squeeze to remove the unnecessary dimension\n",
    "per_token_logps = gathered_values.squeeze(1)\n",
    "print(f\"per_token_logps: {per_token_logps}\")\n",
    "\n",
    "result = torch.mul(per_token_logps, mask)\n",
    "print(f\"result: {result}\")\n",
    "f1 = result.sum(dim=0)\n",
    "f2 = mask.sum(dim=0)\n",
    "print(f\"f1: {f1}\")\n",
    "print(f\"f2: {f2}\")\n",
    "final = f1 / f2\n",
    "print(f\"final: {final}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773c143-3f45-4aea-9189-39f91186adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Iterators and update key variables\n",
    "val_iterator = iter(val_loader)\n",
    "train_iterator = iter(train_loader)\n",
    "log_iters = 100\n",
    "eval_iters= 5 # Use a small number, otherwise things will get too slow\n",
    "\n",
    "print(f\"train loader size: {len(train_loader)}\")\n",
    "print(f\"validation loader size: {len(val_loader)}\")\n",
    "print(f\"number of training steps: {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbf368-73ef-4c1c-aaca-cfa04226e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Prevent gradient calculation\n",
    "# Calculate average of training and validation losses over multiple batches\n",
    "def calculate_loss():\n",
    "    global train_iterator, val_iterator\n",
    "    loss_mean={}\n",
    "    odds_mean={}\n",
    "    ratio_mean={}\n",
    "    model.eval()\n",
    "    for split in ['train','val']: \n",
    "        l=torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        o=torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        r=torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        for i in range(eval_iters):\n",
    "            try:\n",
    "                if split == 'val':\n",
    "                    batch = next(val_iterator)\n",
    "                else:\n",
    "                    batch = next(train_iterator)\n",
    "            except StopIteration:\n",
    "                if split == 'val':\n",
    "                    print(\"####### Resetting Validation Iterator\")\n",
    "                    val_iterator = iter(val_loader)\n",
    "                    batch = next(val_iterator)\n",
    "                else:\n",
    "                    print(\"####### Resetting Training Iterator\")\n",
    "                    train_iterator = iter(train_loader)\n",
    "                    batch = next(train_iterator)                   \n",
    "\n",
    "            batch[\"positive_input_ids\"] = batch[\"positive_input_ids\"].to(device) \n",
    "            batch[\"positive_attention_mask\"] = batch[\"positive_attention_mask\"].to(device)\n",
    "            batch[\"negative_input_ids\"] = batch[\"negative_input_ids\"].to(device)\n",
    "            batch[\"negative_attention_mask\"] = batch[\"negative_attention_mask\"].to(device)\n",
    "            batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "            neg_labels = batch['negative_input_ids'].clone()\n",
    "            pos_labels = batch['positive_input_ids'].clone()\n",
    "        \n",
    "            mask = batch['attention_mask'] * batch['positive_attention_mask']  # sets mask to have 1s in only the prompt positions\n",
    "            pos_labels = pos_labels * mask.logical_not()  # puts 0s where the prompt was, preserve last answer (padding tokens are EOS(2))\n",
    "        \n",
    "            pos_labels[pos_labels == 0] = tokenizer.pad_token_id # replaces 0s with EOS(2)\n",
    "            neg_labels[neg_labels == tokenizer.pad_token_id] = -100 # change 2 to -100 so that loss calculations ignore prompt and padding\n",
    "            pos_labels[pos_labels == tokenizer.pad_token_id] = -100 # change 2 to -100 so that loss calculations ignore prompt and padding\n",
    "        \n",
    "            outputs_pos, loss_pos = model(batch['positive_input_ids'], pos_labels)  #  (1,1024) , (1,1024)\n",
    "            outputs_neg, loss_neg = model(batch['negative_input_ids'], neg_labels)    \n",
    "        \n",
    "            # returns the average of the log probabilities for the positive samples (masking out prompt)\n",
    "            pos_prob = compute_logps(\n",
    "                        prompt_attention_mask=batch['attention_mask'], \n",
    "                        chosen_inputs=batch[\"positive_input_ids\"], \n",
    "                        chosen_attention_mask=batch['positive_attention_mask'], \n",
    "                        logits=outputs_pos\n",
    "                    )\n",
    "            # returns the average of the log probabilities for the negative samples (masking out prompt)\n",
    "            neg_prob = compute_logps(\n",
    "                        prompt_attention_mask=batch['attention_mask'], \n",
    "                        chosen_inputs=batch[\"negative_input_ids\"], \n",
    "                        chosen_attention_mask=batch['negative_attention_mask'], \n",
    "                        logits=outputs_neg\n",
    "                    )    \n",
    "        \n",
    "            # CALCULATE ORPO ODDS RATIO\n",
    "            log_odds = (pos_prob - neg_prob) - (torch.log(1 - torch.exp(pos_prob)) - torch.log(1 - torch.exp(neg_prob)))\n",
    "            sig_ratio = F.sigmoid(log_odds) # constrain to be between 0 and 1\n",
    "            ratio = torch.log(sig_ratio) # apply the final log to the calculation\n",
    "        \n",
    "            # Calculate the Final Total Loss, combination of standard Cross Entropy loss and the weighted Odds Ratio\n",
    "            loss = torch.mean(loss_pos - (alpha*ratio).mean()).to(dtype=dtype)\n",
    "            # notice that mean() is useful if batch size is larger than 1  \n",
    "\n",
    "            l[i]=loss.item()\n",
    "            o[i]=log_odds.mean().item()\n",
    "            r[i]=ratio.mean().item()\n",
    "        \n",
    "        loss_mean[split]=l.mean().item()\n",
    "        odds_mean[split]=o.mean().item()\n",
    "        ratio_mean[split]=r.mean().item()\n",
    "        \n",
    "            \n",
    "    model.train()\n",
    "    return loss_mean, odds_mean, ratio_mean\n",
    "\n",
    "l, o, r = calculate_loss()\n",
    "print(l,o,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3dd71-0ede-4f74-94d2-8f3be0ee40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "################################################\n",
    "############### üéØ ORPO TRAINING ################\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "# üöÄ MAIN TRAINING LOOP - ORPO Alignment Training\n",
    "try:\n",
    "    # üìÖ EPOCH LOOP - Train for multiple complete passes through the dataset\n",
    "    for e in range(epochs):\n",
    "        print(f\"\\nüîÑ Starting Epoch {e+1}/{epochs}\")\n",
    "        \n",
    "        # üì¶ BATCH LOOP - Process each batch of training examples\n",
    "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader), dynamic_ncols=True):\n",
    "        \n",
    "            # üßπ RESET GRADIENTS\n",
    "            optimizer.zero_grad(set_to_none=True)  # Clear gradients from previous iteration\n",
    "    \n",
    "            # üöö MOVE DATA TO DEVICE\n",
    "            # Transfer all batch data to GPU (or CPU) for processing\n",
    "            batch[\"positive_input_ids\"] = batch[\"positive_input_ids\"].to(device) \n",
    "            batch[\"positive_attention_mask\"] = batch[\"positive_attention_mask\"].to(device)\n",
    "            batch[\"negative_input_ids\"] = batch[\"negative_input_ids\"].to(device)\n",
    "            batch[\"negative_attention_mask\"] = batch[\"negative_attention_mask\"].to(device)\n",
    "            batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # üîç DEBUGGING HELPER\n",
    "            # If you want to look inside batch: tokenizer.decode(batch[\"positive_input_ids\"][0])\n",
    "\n",
    "            # üìã PREPARE LABELS FOR TRAINING\n",
    "            # Get the token IDs of positive and negative responses\n",
    "            neg_labels = batch['negative_input_ids'].clone()  # Copy negative response tokens\n",
    "            pos_labels = batch['positive_input_ids'].clone()   # Copy positive response tokens\n",
    "    \n",
    "            # üéØ CALCULATE STANDARD CROSS ENTROPY LOSS (focused on POSITIVE responses)\n",
    "\n",
    "            # üö´ DISABLE LOSS ON PROMPT TOKENS\n",
    "            # When we calculate the standard loss, we focus on the loss of the positive responses\n",
    "            # We want to measure how well the model predicts the next token in the positive, chosen responses\n",
    "            # So we mask the positive IDs to only consider the response tokens, ignoring the prompt\n",
    "            mask = batch['attention_mask'] * batch['positive_attention_mask']  # 1s only in prompt positions\n",
    "            # In our case, this is similar to just mask = batch['attention_mask'] (all sequences have same length)\n",
    "            pos_labels = pos_labels * mask.logical_not()  # 0s where prompt was, preserve response tokens\n",
    "\n",
    "            # üîß PREPARE LABELS FOR LOSS CALCULATION\n",
    "            pos_labels[pos_labels == 0] = tokenizer.pad_token_id  # Replace 0s with EOS token (2)\n",
    "            neg_labels[neg_labels == tokenizer.pad_token_id] = -100  # -100 ignores prompt and padding\n",
    "            pos_labels[pos_labels == tokenizer.pad_token_id] = -100  # -100 ignores prompt and padding\n",
    "\n",
    "            # üß† MODEL FORWARD PASS - POSITIVE RESPONSE\n",
    "            outputs_pos, loss_pos = model(batch['positive_input_ids'], pos_labels)  # Shape: (1,1024), (1,1024)\n",
    "            # positive_input_ids: all token IDs including response and padding (EOS token = 2)\n",
    "            # pos_labels: everything set to -100 except the response tokens (ignored in loss calculation)\n",
    "\n",
    "            # üß† MODEL FORWARD PASS - NEGATIVE RESPONSE  \n",
    "            # We don't use the negative loss for standard training, but we need the output logits\n",
    "            # for the per-token log probability calculations of the negative responses\n",
    "            outputs_neg, loss_neg = model(batch['negative_input_ids'], neg_labels)    \n",
    "\n",
    "            # üßÆ CALCULATE PER-TOKEN LOG PROBABILITIES (necessary for ORPO odds ratio)\n",
    "\n",
    "            # ‚úÖ POSITIVE RESPONSE LOG PROBABILITIES\n",
    "            # Returns the average log probabilities for the positive samples (masking out prompt)\n",
    "            pos_prob = compute_logps(\n",
    "                prompt_attention_mask=batch['attention_mask'], \n",
    "                chosen_inputs=batch[\"positive_input_ids\"], \n",
    "                chosen_attention_mask=batch['positive_attention_mask'], \n",
    "                logits=outputs_pos\n",
    "            )\n",
    "            \n",
    "            # ‚ùå NEGATIVE RESPONSE LOG PROBABILITIES\n",
    "            # Returns the average log probabilities for the negative samples (masking out prompt)\n",
    "            neg_prob = compute_logps(\n",
    "                prompt_attention_mask=batch['attention_mask'], \n",
    "                chosen_inputs=batch[\"negative_input_ids\"], \n",
    "                chosen_attention_mask=batch['negative_attention_mask'], \n",
    "                logits=outputs_neg\n",
    "            )    \n",
    "\n",
    "            # üéØ CALCULATE ORPO ODDS RATIO\n",
    "            # This is the core of ORPO: compare how much the model prefers positive vs negative responses\n",
    "            log_odds = (pos_prob - neg_prob) - (torch.log(1 - torch.exp(pos_prob)) - torch.log(1 - torch.exp(neg_prob)))\n",
    "            sig_ratio = F.sigmoid(log_odds)  # Constrain to be between 0 and 1\n",
    "            ratio = torch.log(sig_ratio)     # Apply final log to the calculation\n",
    "\n",
    "            # üéØ CALCULATE FINAL TOTAL LOSS\n",
    "            # Combination of standard Cross Entropy loss and the weighted Odds Ratio\n",
    "            loss = torch.mean(loss_pos - (alpha*ratio).mean()).to(dtype=dtype)\n",
    "            # Note: mean() is useful if batch size is larger than 1\n",
    "\n",
    "            # üìä LOGGING AND MONITORING\n",
    "            if i%log_iters == 0:\n",
    "                # Calculate average losses across multiple batches for more stable metrics\n",
    "                loss_m, log_odds_m, ratio_m = calculate_loss()\n",
    "\n",
    "                # üìà PRINT TRAINING PROGRESS\n",
    "                print(f\"Epochs [{e+1}/{epochs}] Step: [{i}/{len(train_loader)}], train loss: {loss_m['train']:.4f}, val loss: {loss_m['val']:.4f}, Odds Ratio: {log_odds_m['train']:.4f}, val Odds Ratio: {log_odds_m['val']:.4f}\")\n",
    "                \n",
    "                # üìä WANDB LOGGING\n",
    "                if wandb_log:\n",
    "                    wandb.log({\n",
    "                        \"train_loss\": loss_m['train'],           # Training loss\n",
    "                        \"val_loss\": loss_m['val'],               # Validation loss  \n",
    "                        \"train_log_odds\": log_odds_m['train'],   # Training odds ratio\n",
    "                        \"val_log_odds\": log_odds_m['val'],       # Validation odds ratio\n",
    "                        \"train_ratio\": (alpha*ratio_m['train']),  # Weighted training ratio\n",
    "                        \"val_ratio\": (alpha*ratio_m['val']),      # Weighted validation ratio\n",
    "                        # Optional metrics (uncomment to track):\n",
    "                        #\"pos_prob\": pos_prob.mean().item(),     # Positive response probability\n",
    "                        #\"neg_prob\": neg_prob.mean().item(),     # Negative response probability                        \n",
    "                        #\"lr\": scheduler.get_last_lr()[0],       # Current learning rate\n",
    "                    }, \n",
    "                    step = (e*len(train_loader) + i))\n",
    "\n",
    "                # üö® NAN DETECTION\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"‚ùå NaN loss detected! Stopping training...\")\n",
    "                    if wandb_log:   \n",
    "                        wandb.finish()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    sys.exit()\n",
    "\n",
    "            # üîÑ OPTIMIZATION STEP\n",
    "            loss.backward()  # Calculate gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)  # Clip gradients to prevent explosion\n",
    "            optimizer.step()  # Update model parameters\n",
    "            scheduler.step()  # Update learning rate\n",
    "\n",
    "        # üíæ SAVE CHECKPOINT AT END OF EPOCH\n",
    "        # Save model state and configuration for future use\n",
    "        sd = model.state_dict()\n",
    "        sd['config'] = config\n",
    "        torch.save(sd, os.path.join(checkpoint_dir, f'{project_name}_{e+1}.pt'))\n",
    "        print(f\"‚úÖ Checkpoint saved: {project_name}_{e+1}.pt\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"‚èπÔ∏è Training interrupted by user. Cleaning up...\")\n",
    "\n",
    "finally:\n",
    "    # üßπ CLEANUP AND MEMORY MANAGEMENT\n",
    "    # Release GPU memory to free up resources\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ GPU memory released.\")\n",
    "\n",
    "# üìä FINALIZE LOGGING\n",
    "if wandb_log:   \n",
    "    wandb.finish()\n",
    "    print(\"üìä Weights & Biases logging finished.\")\n",
    "\n",
    "# üßπ FINAL CLEANUP\n",
    "torch.cuda.empty_cache()\n",
    "print(\"üéâ ORPO training completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d92ddf-2782-4ff5-a1b5-508639b408c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
